---
title: "Progetto HDDA AA 2021/2022"
author: "Berto, Sauro, Venturi"
date: "9 dicembre 2021"
output: 
  html_notebook:
    toc: true 
    toc_depth: 3 
---

# Introduzione

Il *Social Lending* è una forma di prestito erogato da privati per privati via Internet che nasce in Gran Bretagna nel 2005 e si afferma definitivamente nel mercato finanziario a seguito della crisi del 2008. Uno dei principali intermediari al mondo di Social Lending è la piattaforma *LendingClub*, che a partire dalla fondazione nel 2006 in soli 9 anni ha gestito 16 miliardi di dollari. \
Con il presente lavoro si applicano i metodi di Statistical Learning ai dati messi a disposizione dalla piattaforma, con l'obiettivo di allenare modelli in grado di riconoscere i prestiti *"buoni"*, ovvero che saranno ripagati, da quelli *"cattivi"*. Si pongono così le basi per la creazione di un prodotto che suggerisca al cliente delle strategie di investimento personalizzate, che tengano conto in maniera trasparente della propensione al rischio dell'investitore.

I modelli che verranno testati sono:

- Regressione Logistica 
  + base
  + con PCA
  + con Backward Elimination
  + con penalizzazione Ridge
  + con penalizzazione Lasso 
- Random Forest 
- Modello di Cox 

```{r include=F}
if(!require(data.table)) install.packages("data.table")
if(!require(tibble)) install.packages("tibble")
if(!require(Hmisc)) install.packages("Hmisc")
if(!require(dplyr)) install.packages("dplyr")
if(!require(ggplot2)) install.packages("ggplot2")
if(!require(scales)) install.packages("scales")
if(!require(tidyr)) install.packages("tidyr")
if(!require(corrplot)) install.packages("corrplot")
if(!require(lubridate)) install.packages("lubridate")
if(!require(stringr)) install.packages("stringr")
if(!require(sampling)) install.packages("sampling")
if(!require(leaps)) install.packages("leaps")
if(!require(bestglm)) install.packages("bestglm")
if(!require(glmnet)) install.packages("glmnet")
if(!require(factoextra)) install.packages("factoextra")
if(!require(caret)) install.packages("caret")
if(!require(ranger)) install.packages("ranger")
if(!require(randomForest)) install.packages("randomForest")
if(!require(InformationValue)) install.packages("InformationValue")
if(!require(kernlab)) install.packages("kernlab")
if(!require(e1071)) install.packages("e1071")
if(!require(survival)) install.packages("survival")
if(!require(survminer)) install_github("kassambara/survminer", build_vignettes = FALSE)
if(!require(riskRegression)) install.packages("riskRegression")
if(!require(parallel)) install.packages("doParallel")
if(!require(doMC)) install.packages("doMC")
if(!require(patchwork)) install.packages("patchwork")
if(!require(reshape2)) install.packages("reshape2")

library(data.table)
library(tibble)
library(Hmisc)
library(tidyr)
library(dplyr)
library(ggplot2)
library(corrplot)
library(lubridate)
library(stringr)
library(sampling)
library(leaps)
library(bestglm)
library(glmnet)
library(factoextra)
library(caret)
library(ranger)
library(randomForest) 
library(InformationValue) 
library(kernlab) 
library(e1071) 
library(survival)
library(riskRegression)
library(parallel)
library(doMC)
library(pander)
library(patchwork)
library(car)

`%notin%` <- Negate(`%in%`)

registerDoMC(cores = detectCores())

source("data_preparation/CustomFunctions.R")
```

# Data Ingestion

Il dataset utilizzato proviene da [kaggle](https://www.kaggle.com/wordsforthewise/lending-club). \
Le osservazioni del dataset rappresentano i prestiti richiesti tramite la piattaforma nel periodo 2007-2018, di questi vengono selezionati solo i prestiti che sono stati erogati (contenuti nel file: accepted_2007_to_2018Q4.csv) e si ottengono così 2,260,701 osservazioni. \
Il numero di attributi ammonta a 151, essi descrivono le caratteristiche del prestito, tra cui: la formulazione della richiesta, la situazione finanziaria del richiedente, l'esito del prestito, ecc.

```{r results='hide'}
acc_full <- fread("archive/accepted_2007_to_2018Q4.csv", sep = ",", header = T)
```

Non tutti i 151 attributi sono utilizzabili poichè vengono aggiornati periodicamente dalla piattaforma, pertanto alcuni di essi tengono conto dell'evoluzione del prestito, informazione che ovviamente non può essere utilizzata per la costruzione di modelli di classificazione. Dopo aver studiato ciascuno dei 151 attributi ne sono stati infine selezionati 40.

Attributo|Tipo|Significato
-|-|-
addr_state|Factor|Stato di provenienza del richiedente (USA)
annual_inc|Int|Reddito annuale fornito dal richiedente
application_type|Factor|Tipo di richiesta: individuale o congiunta
delinq_2yrs|Int|Numero di insolvenze rateali superiori a 30 giorni segnalate negli ultimi 2 anni nel registro di credito del richiedente
dti|Float|Rapporto tra debiti e reddito del richiedente
earliest_cr_line|Date|Data in cui il richiedente ha aperto la sua prima linea di credito
emp_length|Int|Durata dell'impiego in anni. Il valore varia tra 0 e 10, dove 10 indica 10 o più anni
fico_range_high|Factor|Estremo superiore dell'intervallo FICO del richiedente
fico_range_low|Factor|Estremo inferiore dell'intervallo FICO del richiedente
grade|Factor|Score assegnato da parte di LendingClub al richiedente
home_ownership|Factor|Indica se il richiedente è proprietario di un immobile o è in affitto
initial_list_status|Factor|Copertura del prestito da parte di un singolo investitore o meno
int_rate|Float|Tasso annuale del prestito
issue_d|Date|Data di emissione dei fondi (mese-anno)
last_pymnt_d|Date|Data dell'ultimo pagamento (mese-anno)
loan_amnt|Int|Importo richiesto
funded_amnt|Int|Importo erogato
**loan_status**|Factor|Stato del prestito (in corso, default, in ritardo, ...), variabile target.
mo_sin_old_il_acct|Int|Mesi trascorsi dalla più vecchia linea di credito bancaria
mo_sin_old_rev_tl_op|Int|Mesi trascorsi dall'apertura del più vecchio conto revolving
mort_acc|Int|Numero di mutui intestati al richiedente
num_bc_sats|Int|Numero di solvenze degli estratti conto delle carte di credito del cliente
num_bc_tl|Int|Numero delle carte del cliente
num_op_rev_tl|Int|Numero di carte revolving del cliente
num_rev_accts|Int|Numero di conti revolving
num_rev_tl_bal_gt_0|Int|Numero di transazioni revolving con saldo maggiore di 0
num_sats|Int|Numero di linee di credito solventi intestate al richiedente
open_acc|Int|Numero di linee di credito aperte
pct_tl_nvr_dlq|Int|Percentuale di rinnovi di credito solventi
percent_bc_gt_75|Int|Percentuale di carte di credito sopra al massimale di oltre il 75%
pub_rec|Int|Numero di ritardi di pagamento
pub_rec_bankruptcies|Factor|Numero di volte in cui il cliente ha dichiarato bancarotta
purpose|Factor|Motivo del prestito, espresso dal richiedente
revol_bal|Int|Saldo totale del credito revolving del richiedente
revol_util|Float|Percentuale di utilizzo del credito revolving da parte del richiedente
sub_grade|Factor|Score assegnato da parte di LendingClub al richiedente
tax_liens|Int|Numero di ipoteche intestate al richiedente
term|Int|Numero di rate, durata del prestito
total_acc|Int|Numero di prestiti in corso al momento della richiesta
verification_status|Factor|Verifica delle informazioni fornite dal richiedente

```{r cache=TRUE, include=F}
variables <- c(
  "addr_state",
  "annual_inc",
  "dti",
  "application_type",
  "delinq_2yrs",
  "earliest_cr_line",
  "emp_length",
  "fico_range_high",
  "fico_range_low",
  "grade",
  "home_ownership",
  "initial_list_status",
  "int_rate",
  "issue_d",
  "last_pymnt_d",
  "loan_amnt",
  "funded_amnt",
  "loan_status",
  "mo_sin_old_il_acct",
  "mo_sin_old_rev_tl_op",
  "mort_acc",
  "num_bc_sats",
  "num_bc_tl",
  "num_op_rev_tl",
  "num_rev_accts",
  "num_rev_tl_bal_gt_0",
  "num_sats",
  "open_acc",
  "pct_tl_nvr_dlq",
  "percent_bc_gt_75",
  "pub_rec",
  "pub_rec_bankruptcies",
  "purpose",
  "revol_bal",
  "revol_util",
  "sub_grade",
  "tax_liens",
  "term",
  "total_acc",
  "verification_status"
)
```

```{r results='hide'}
acc <- acc_full
acc <- acc[,..variables]
acc <- acc[complete.cases(acc),]
acc <- acc[!apply(acc, 1, function(x) any(x=="")),]
fwrite(acc, "archive/accepted_clean.csv", sep = ",")
```

# Data Exploration e Preparation

Conservati gli attributi utili allo scopo ed eliminate le osservazioni con valori mancanti, si procede alla fase di esplorazione e pulizia del dataset. Per gli attributi numerici è stato scelto di eliminare gli outliers oltre la soglia di $3\sigma$ dalla media, mentre per gli attributi categorici è stata eseguita un'analisi individuale, analizzandone la numerosità dei livelli e riducendola ove fosse eccessiva. \
In particolare l'attributo sullo stato di provenienza del richiedente (*addr_state*), è stato portato da 52 livelli a 7, rappresentanti le macro-aree geografiche degli Stati Uniti (West Coast, MidWest, ...). L'attributo *purpose* è stato portato da 15 livelli a 3, mantenendo i 2 più numerosi e accorpando tutti gli altri in *other*, eliminando così livelli presenti in poche migliaia di osservazioni.
L'attributo *home_ownership* contiene dei livelli con poche centinaia di osservazioni, pertanto queste vengono rimosse.
L'attributo *emp_length*, inizialmente numerico, è stato convertito in un fattore a tre livelli: low, mid, high.
Infine l'attributo *loan_status* viene trasformato in un fattore a 2 livelli: 0 se il prestito è in stato di default o in forte ritardo, 1 se è stato rimborsato con successo.
I prestiti ancora in corso e non in forte ritardo sono invece stati rimossi.

Gli attributi di tipo Date sono stati convertiti in attrbuti numerici espressi come "numero di mesi da tale data fino a dicembre 2018", per conformità ad altri attributi numerici già espressi in tale modo.

```{r results='hide'}
acc <- fread("archive/accepted_clean.csv", sep=",",header=T)
acc <- as.data.frame(acc)
numeric_variables <- c(
  "annual_inc",
  "dti",
  "delinq_2yrs",
  "mo_sin_earliest_cr_line",
  "fico_range_high",
  "fico_range_low",
  "int_rate",
  "loan_amnt",
  "funded_amnt",
  "mo_sin_old_il_acct",
  "mo_sin_old_rev_tl_op",
  "mort_acc",
  "num_bc_sats",
  "num_bc_tl",
  "num_op_rev_tl",
  "num_rev_accts",
  "num_rev_tl_bal_gt_0",
  "num_sats",
  "open_acc",
  "pct_tl_nvr_dlq",
  "percent_bc_gt_75",
  "pub_rec",
  "pub_rec_bankruptcies",
  "revol_bal",
  "revol_util",
  "tax_liens",
  "total_acc"
)

acc <- acc %>%
  mutate(
    mo_sin_earliest_cr_line=as.integer(round((as.Date("2018-12-01")-lubridate::my(earliest_cr_line))/(365.25/12))),
    issue_d = lubridate::my(issue_d),
    last_pymnt_d = lubridate::my(last_pymnt_d),
    emp_length=as.numeric(str_extract(emp_length, "[0-9]+"))
  ) %>%
  mutate(
    emp_length = case_when(
      emp_length < 5 ~ "low",
      emp_length < 10 ~ "mid",
      T ~"high")) %>%
  select(-earliest_cr_line)

acc_with_current <- acc

acc <- acc %>% 
  filter(loan_status %notin% c("Current", "In Grace Period", "Late (16-30 days)")) %>%
  mutate(y=case_when(
    loan_status == "Late (31-120 days)" ~ 0,
    loan_status == "Charged Off" ~ 0,
    loan_status == "Default" ~ 0,
    loan_status == "Fully Paid" ~ 1
  )) %>% 
  mutate(addr_state=case_when(
    addr_state %in% c("WA", "OR", "CA", "HI", "AL", "AK") ~ "West Coast",
    addr_state %in% c("MT", "ID", "NV", "UT", "CO", "WY") ~ "The Rocky Mountains",
    addr_state %in% c("AZ", "NM", "TX", "OK") ~ "South West",
    addr_state %in% c("ND", "SD", "NE", "KS", "MN", "IA", "MO", "WI", "IL", "IN", "MI", "OH") ~ "Midwest",
    addr_state %in% c("AR", "LA", "MS", "KY", "TN", "AL", "GA", "FL", "NC", "SC", "VA", "WV") ~ "South East",
    addr_state %in% c("ME", "VT", "NH", "MA", "RI", "CT") ~ "New England",
    addr_state %in% c("NY", "PA", "NJ", "MD", "DE", "DC") ~ "Mid Atlantic",
  )) %>%
  # livelli erano molto sbilanciati, optato per 3 livelli
  mutate(purpose=case_when(
    purpose %in% c("debt_consolidation") ~ "debt_consolidation",
    purpose %in% c("credit_card") ~ "credit_card",
    T ~ "other"
  )) %>% 
  mutate(
    addr_state=as.factor(addr_state),
    y=as.factor(y),
    application_type=as.factor(application_type),
    emp_length=as.factor(emp_length),
    grade=as.factor(grade),
    home_ownership=as.factor(home_ownership),
    initial_list_status=as.factor(initial_list_status),
    purpose=as.factor(purpose),
    sub_grade=as.factor(sub_grade),
    term=as.factor(term),
    verification_status=as.factor(verification_status)
  ) %>% 
  filter(home_ownership %notin% c("ANY", "NONE", "OTHER"))  %>%
  mutate(home_ownership = as.factor(as.character(home_ownership))) %>% # rimozione livelli senza osservazioni
  select(-loan_status)
set.seed(1000)
source("data_preparation/Outliers.R", print.eval = T)
```

A seguito di questa prima pulizia vengono presentate alcune visualizzazioni utili alla comprensione del contesto.

## Numero di osservazioni / tempo 

```{r fig.height=6, fig.width=10, message=FALSE, warning=FALSE}
month_summary_with_current <- 
  acc_with_current %>% 
  group_by(issue_d) %>% 
  summarise(n_current= n())

month_summary <- 
  acc %>% 
  group_by(issue_d) %>% 
  summarise(n = n())

month_summary_full <- 
  acc_full %>% 
  mutate(issue_d = lubridate::my(issue_d)) %>%
  group_by(issue_d) %>% 
  summarise(n_full= n()) %>%
  left_join(month_summary_with_current) %>%
  left_join(month_summary) 

plotly::plot_ly(month_summary_full, x = ~issue_d) %>% 
  plotly::add_trace(y = ~n_full, mode = "lines+markers", name = "All", opacity = 0.2) %>%
  plotly::add_trace(y = ~n_current, mode = "lines+markers", name = "Senza Missing Values", opacity = 0.5) %>%
  plotly::add_trace(y = ~n, mode = "lines+markers", name = "Senza Missing Values e Current", opacity = 1) %>%
  plotly::layout(title = "Crescita di LendingClub",  xaxis = list(title = 'Data'),
                 yaxis = list(title = 'Numero prestiti finanziati'),
                 legend = list(x=0.1,y=0.9))
```


```{r include=F}
rm(acc_full)
rm(acc_with_current)
```

Si osserva la forte espansione della piattaforma che a partire dal 2012 assume un carattere lineare, per arrivare negli anni più recenti a oltre 40,000 prestiti erogati al mese.
Si nota come prima di agosto 2012 non siano presenti osservazioni senza valori mancanti, inoltre si nota come molti dei prestiti più recenti siano ancora in corso, poichè ciò non ci consente di conoscere il loro esito, tali prestiti sono esclusi dall'analisi.

## Distribuzione Loan Status

```{r}
piechart(acc,group="y") +
  scale_fill_manual(labels=c("Default", "Fully Paid"), values=c("darkred","darkgreen")) + ggtitle("Pie chart Lending Outcome") + theme(plot.title = element_text(hjust = 0.5, size = 16))
```

Si osserva come il dataset sia fortemente sbilanciato, infatti oltre il 79% dei prestiti erogati viene ripagato. Questo aspetto dovrà essere tenuto di conto in fase di costruzione dei modelli.

## Altre Distribuzioni

```{r fig.height=6, fig.width=10, message=FALSE, warning=FALSE}
source("data_preparation/AllViolins.R", print.eval = T)
```

Osservando le distribuzioni degli attributi numerici si nota come la rimozione degli outliers abbia avuto successo, tuttavia sono presenti distribuzioni fortemente asimmetriche. Anche alcuni attributi categorici presentano sbilanciamento dei livelli.

## Analisi della correlazione

L'ultimo quesito dell'analisi esplorativa è l'eventuale presenza di correlazione tra gli attributi.

```{r , out.width= '100%'}
cor1 <- MyCorrelationMatrix(acc[,numeric_variables])
knitr::include_graphics(cor1)
```

Dalla matrice di correlazione si osserva la presenza di un elevato numero di attributi tra loro correlati, in particolare quelli relativi alla situazione finanziaria del richiedente (conti, carte, linee di credito, ...). Poichè la presenza di tale correlazione ci condurrebbe a significatività fittizie occorre eliminarla.
Gli attributi *fico_range_low* e *fico_range_high* hanno distanza costante su tutte le osservazioni, se ne estrae quindi la media e si procede infine a riassumerli in 5 livelli come di consuetudine ([link](https://www.investopedia.com/terms/f/ficoscore.asp)).
Le coppie di attributi (*num_sats*, *open_acc*) e (*num_bc_sats*, *num_bc_tl*) hanno forte scorrelazione, si decide pertanto di considerare il numero di conti/carte non soddisfacenti piuttosto che quelli soddisfacenti.

```{r}
### categorizzazione fico_mean (https://www.investopedia.com/terms/f/ficoscore.asp)
acc <- acc %>% 
  mutate(fico_mean = (fico_range_high+fico_range_low)/2) %>%
  select(-fico_range_high, -fico_range_low) %>% 
  mutate(fico_class=case_when(
    fico_mean <= 580 ~ "Poor",
    fico_mean > 580 & fico_mean <= 669 ~ "Fair",
    fico_mean > 670 & fico_mean <= 739 ~ "Good",
    fico_mean > 740 & fico_mean <= 799 ~ "Very Good",
    fico_mean > 800 ~ "Exceptional"
  )) %>% select(-fico_mean) %>% mutate(fico_class=as.factor(fico_class))

acc <- acc %>% 
  mutate(num_not_sats = (open_acc - num_sats)/open_acc,
         num_bc_not_sats = (num_bc_tl - num_bc_sats)/num_bc_tl) %>%
  select(-num_sats, num_bc_sats)

# tolte le correlazioni, restano queste variabili esplicative numeriche
new_numeric_variables <- c(
  "annual_inc",
  "dti",
  "delinq_2yrs",
  "mo_sin_earliest_cr_line",
  "int_rate",
  "loan_amnt",
  "mo_sin_old_il_acct",
  "mo_sin_old_rev_tl_op",
  "mort_acc",
  "num_bc_not_sats",
  "num_bc_tl",
  "num_op_rev_tl",
  "num_rev_accts",
  "num_rev_tl_bal_gt_0",
  "num_not_sats",
  "open_acc",
  "pct_tl_nvr_dlq",
  "percent_bc_gt_75",
  "pub_rec",
  "pub_rec_bankruptcies",
  "revol_bal",
  "revol_util",
  "tax_liens",
  "total_acc"
)
cor2 <- MyCorrelationMatrix(acc[,new_numeric_variables])
knitr::include_graphics(cor2)
```

Permangono ancora alcuni attributi correlati che vengono quindi rimossi: *pub_rec_bankruptcies*, *num_rev_accts*, *mo_sin_old_rev_tl_op*. Si rimuovono inoltre *grade* e *sub_grade* essendo degli score attribuiti dalla piattaforma potenzialmente aggiornati a seguito dell'evoluzione del prestito.

```{r}
model_variables <-  names(acc)[names(acc) %notin% c(
  "grade",
  "funded_amnt", # non nota al momento dell'utilizzo dei modelli
  "issue_d",
  "last_pymnt_d",
  "num_sats",
  "num_bc_sats",
  "sub_grade",
  "pub_rec_bankruptcies",
  "num_rev_accts",
  "mo_sin_old_rev_tl_op"
)]

model_num_variables <- model_variables[model_variables %in% new_numeric_variables]

model_acc <- acc[,model_variables] %>% relocate(y, .after = last_col())
```

# Modeling

Si procede alla costruzione di training set e test set, con metodo hold-out e rapporto 70-30. Inoltre si normalizzano le variabili numeriche.

```{r message=F}
set.seed(1000)
train <- sample_frac(model_acc, 0.7)
test <- anti_join(model_acc,train)

scaling <- preProcess(train, method = c("center", "scale"))
train <- predict(scaling,train)
test <- predict(scaling, test)

train.under <- train %>% group_by(y) %>% slice_sample(n=min(table(train$y))) %>% ungroup()
```

Per trattare lo sbilanciamento della variabile target si costruiscono dei pesi a seconda del numero di osservazioni.

```{r}
model_weights <- ifelse(train$y == "0",
                        (1/table(train$y)[1]) * 0.5, ### (1 / numero osservazioni classe 0) / 2
                        (1/table(train$y)[2]) * 0.5) ### (1 / numero osservazioni classe 1) / 2
```

## Logistic Regression

### Base

```{r}
fit.glm.unbalanced <- glm(y~., train, family="binomial")
yhat.glm.unbalanced <- predict(fit.glm.unbalanced, test, type="response")
cat("Accuracy del Modello Unbalanced:", 1-misClassError(test$y, yhat.glm.unbalanced))
```

```{r}
set.seed(1000)
fit.glm.under <- glm(y~., train.under, family="binomial")
yhat.glm.under<- predict(fit.glm.under, test, type="response")
cat("Accuracy of UnderSampling Model:", 1-misClassError(test$y, yhat.glm.under))
```

```{r}
fit.glm.weighted <- glm(y~., train, family="binomial", weights=model_weights)
yhat.glm.weighted <- predict(fit.glm.weighted, test, type="response")
cat("Accuracy del Modello Balanced:", 1-misClassError(test$y, yhat.glm.weighted))
```

```{r fig.width=10, fig.height=4}
p1 <- MyConfusionMatrix(test$y, yhat.glm.unbalanced, title="Logit Unbalanced")
p2 <- MyConfusionMatrix(test$y, yhat.glm.under, title="Logit UnderSampling")
p3 <- MyConfusionMatrix(test$y, yhat.glm.weighted, title="Logit Weighted")

p1+p2+p3
```

```{r fig.width=10, fig.height=6}
plot_precision_at_k(test$y, list("Logit Unbalanced"=yhat.glm.unbalanced, "Logit Undersampling"=yhat.glm.under, "Logit Weighted"=yhat.glm.weighted))
```

Il bilanciamento può essere eseguito tramite due approcci: l'utilizzo dei pesi o l'undersampling, ciò ci permetterà di avere previsioni più attendibili sulla classe minoritaria (*Default*), al costo tuttavia di peggiorare l'accuracy totale del nostro modello. Nonostante il guadagno sia minimo, come si nota nell'ultimo grafico osservando la distanza tra la curva verde e le altre due, esso è presente e ci guida così verso la scelta di un train set bilanciato. Il bilanciamento viene infine eseguito con tecnica di undersampling poichè porta un risultato perfettamente comparabile con l'utilizzo dei pesi ma a un costo computazionale nettamente inferiore.

```{r}
InformationValue::plotROC(test$y, yhat.glm.under)
```

Il modello di Regressione Logistica con undersampling ha un valore di AUC pari a 70.4%, questo è il punto partenza, infatti con tale valore confronteremo i modelli successivi per avere un'indicazione di quale sia il migliore.

```{r}
train <- train.under
```

Si osservi la scelta di utilizzare il metodo Hold-Out per ottenere le previsioni invece della più diffusa CrossValidation ($k\ge3$), ciò è stato fatto consapevolmente al fine di contenere il costo computazionale.
Avendo a disposizione un dataset di circa 800,000 osservazioni, senza outliers, la creazione di, ad esempio, 20 folds avrebbe condotto a modelli molto robusti in fase di training (~270,000 ossservazioni nel training set) e molto stabili anche in fase di test (~14,000 osservazioni nel test set). \
Per riprova possiamo mostrare questo effetto utilizzando una CrossValidation 20-folds sul Modello Logit base.

```{r}
K <- 20
folds <- sample(rep(1:K, length=nrow(train)))
accs <- c()
precs <- c()
for(k in 1:K){
  fit <- glm(y ~ ., train[which(folds!=k),], family="binomial")
  x.out=train[which(folds==k),] %>% select(-y) ### lascio fuori la "cartella" k-esima per la fase di training
  yhat.cv=predict(fit, x.out, type="response") ### predico sulla "cartella" k-esima
  y.out=train$y[which(folds==k)]
  accs[k] <- 1-misClassError(y.out, yhat.cv, threshold = 0.8)
  precs[k] <- precision(y.out, yhat.cv, threshold = 0.8)
}

cat("Accuracy al 95% C.I per 20-fold Crossvalidation:", round(mean(accs),5), "±", round(1.960*sd(accs)/sqrt(nrow(train)/K),5),"\n")

cat("Precision al 95% C.I per 20-fold Crossvalidation:", round(mean(precs),5), "±", round(1.960*sd(precs)/sqrt(round(nrow(train))/K),5))
```

Si ottiene effettivamente una previsione estramemente stabile, ad esempio con threshold a 0.8:
- Accuracy con C.I. al 95%: $(53.114 \pm 0.006)\%$
- Precision con C.I. al 95%: $(87.27 \pm 0.02)\%$

Non essendoci motivo di ritenere che gli altri modelli assumano un comportamento diverso, si procede utilizzando il metodo hold-out.

### PCA

```{r}
pca.res <- prcomp(train[, model_num_variables])

p1 <- ggplot()+
  geom_point(aes(x=1:length(pca.res$sdev), y=pca.res$sdev, col=pca.res$sdev<1)) +
  geom_line(aes(x=1:length(pca.res$sdev),y=1), color="red") + 
  geom_text(aes(x=1:length(pca.res$sdev), y=pca.res$sdev+0.1, label=1:length(pca.res$sdev))) +
  scale_color_manual(values = c("red", "blue"), guide="none") +
  labs(x="Component", y="Standard Deviation", title = "PCA components") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, size = 16))

p2 <- fviz_eig(pca.res, chioce="eigenvalue", ggtheme = theme_minimal() + theme(plot.title = element_text(hjust = 0.5, size = 16)))
p1+p2
```

Utilizziamo la tecnica della PCA per ridurre il numero di predittori e al contempo eliminare l'eventuale multi-collinearità residua. Vengono selezionati le componenti principali con deviazione standard maggiore di 1.0, poichè esse racchiudono più informazione delle variabili di partenza. Le otto componenti così selezionate spiegano il ~71% della varianza totale.

```{r}
train.pca <- as.data.frame(as.matrix(train[,model_num_variables]) %*% pca.res$rotation)[,1:8] # prime otto variabili
test.pca <- as.data.frame(as.matrix(test[,model_num_variables]) %*% pca.res$rotation)[,1:8] # prime otto variabili

train.pca$y <- train$y
test.pca$y <- test$y
```

```{r}
fit.pca <- glm(y~., train.pca, family="binomial")

yhat.pca <- predict(fit.pca, test.pca, type="response")

MyConfusionMatrix(test.pca$y, yhat.pca, title = "Logit - PCA")

cat("Accuracy del Modello Logit con PCA:", 1-misClassError(test.pca$y, yhat.pca))
```

```{r}
InformationValue::plotROC(test.pca$y, yhat.pca)
```

La tecnica della PCA ci consente di ottenere un modello più parsimonioso avendo ridotto notevolmente il numero di regressori, tuttavia i risultati sono considerevolmente peggiori.

### Backward Elimination

Un'altra tecnica utile al contenimento del costo computazionale del modello è la Best Subset Selection, ovvero la selezione del miglior subset di regressori, nel nostro caso valutata sull'AIC. 

```{r results='hide'}
fit.glm <- glm(y~., train, family="binomial")

glm.backward <- step(fit.glm, direction = "backward", trace=T, k=2)

fit.glm.backward <- glm(glm.backward$formula, train, family = "binomial")

yhat.glm.backward <- predict(fit.glm.backward, test, type="response")
```

```{r}
all_covariates <- model_variables[model_variables != "y"]
backward_selection <- str_extract_all(glm.backward$formula, pattern = "[a-z|_]+")[[3]]

cat("Le variabili eslcuse dalla Backward Step-wise Selection sono:\n", paste(all_covariates[all_covariates %notin% backward_selection], collapse="\n "))
```

La Backward Step-wise Selection esclude 7 variabili dall'insieme dei regressori dopodiché il processo è stato interrotto dall'*AIC stopping rule* non essendo stato osservato un miglioramento in termini di AUC. Si è ottenuto così un modello più parsimonioso rispetto alla Regressione Logistica senza feature selection.

```{r}
MyConfusionMatrix(test$y, yhat.glm.backward, title = "Logit - Backward Elimination")

cat("Accuracy del Modello Logit con Backward Elimination:", 1-misClassError(test$y, yhat.glm.backward))
```

```{r}
InformationValue::plotROC(test$y, yhat.glm.backward)
```

Si osserva come l'AUC sia la stessa del modello base, tuttavia questo modello resta preferibile poichè più parsimonioso.

### Ridge

Proviamo anche l'approccio di regressione penalizzata, con i metodi non-parametrici Ridge e Lasso.

```{r}
X.train <- model.matrix(y~., train)[,-1]
K <- 10
fit.ridge.cv <- cv.glmnet(X.train, train$y, alpha=0, family="binomial", nfolds = K, grouped = FALSE, type.measure = "class", parallel = T)
plot(fit.ridge.cv)
```

Per quanto riguarda la penalizzazione Ridge si seleziona il lambda a $1\sigma$ dal lambda "minimo" per ridurre la varianza del modello il più possibile.

```{r}
X.test <- model.matrix(y~., test)[,-1]

yhat.ridge = predict(fit.ridge.cv, X.test, s = "lambda.1se",  type="response")

MyConfusionMatrix(test$y, yhat.ridge, threshold = 0.5, title = "Ridge Logit")
cat("Accuracy del Modello Logit con penalizzazione Ridge:", 1-misClassError(test$y, yhat.ridge, threshold = 0.5))
```

```{r}
InformationValue::plotROC(test$y, yhat.ridge)
```

L'AUC ci indica che siamo in presenza di un modello leggermente peggiore del modello base e del modello con backward elimination. 

### Lasso

Ripetiamo quanto visto con la penalizzazione lasso.

```{r}
K <- 10
fit.lasso.cv <- cv.glmnet(X.train, train$y, alpha=1, family="binomial", nfolds = K, grouped = FALSE, type.measure = "class", parallel = T)
plot(fit.lasso.cv)
```

Utilizzando sempre il lambda a $1\sigma$ dal lambda "minimo" si ottiene un modello che scarta 8 variabili per intero e alcuni livelli di altre 2 variabili categoriche (*addr_state* e *purpose*).

```{r}
cat(paste(c("Le variabili escluse dalla selezione Lasso sono:", rownames(coef(fit.lasso.cv, s = "lambda.1se"))[(coef(fit.lasso.cv, s = "lambda.1se") == 0)[,1]]), collapse=" \n " ))
```

```{r}
yhat.lasso <-  predict(fit.lasso.cv, X.test, s = "lambda.1se",  type="response")

MyConfusionMatrix(test$y, yhat.lasso, threshold = 0.5, title="Lasso Logit")
cat("Accuracy del Modello Logit con penalizzazione Lasso:", 1-misClassError(test$y, yhat.lasso, threshold = 0.5))
```

```{r}
InformationValue::plotROC(test$y, yhat.lasso)
```

L'AUC è molto simile a quella dei migliori modelli già visti ma in questo caso stiamo eliminando ben 8 variabili (e anche qualche livello di di due variabili categoriche), quindi questo modello è sicuramente il migliore per rapporto tra performance e costo computazionale.

## Random Forest

Si prova adesso un modello di Random Forest. Dapprima si ricerca il parametro *mtry* migliore, ovvero il miglior numero di regressori che deve contenere ciascun albero, utilizzando un'architettura con 50 alberi e sottocampionando il training set per rendere più veloce la computazione. Ottenuto il valore ottimale di *mtry* si costruisce un Random Forest con 200 alberi e si riaumenta la dimensione del training set. \
Entrambi i Random Forest utilizzano la tecnica *Bootstrap* per ricampionare il training set per ciascun albero e la dimensione di ciascun campione bootstrap coincide con la dimensione del training set. Questa tecnica permette di ottenere alberi più incorrelati tra di loro e di conseguenza di ridurre il termine di varianza dell'errore.
Infine viene utilizzata la tecnica del *Bagging* per ottenere i risultati finali a partire da quelli di ciascuno dei 200 alberi.

```{r}
customTuneRF<- function (data, ntree, mtryStart){
  data <- data %>% group_by(y) %>% slice_sample(n=1e5) %>% ungroup() 
  x <- data %>% select(-y)
  y <- data$y
  tuning_data <- data %>% group_by(y) %>% slice_sample(n=5e4) %>% ungroup()
  tuning_x <- tuning_data %>% select(-y)
  tuning_y <- tuning_data$y
  tune <- tuneRF(tuning_x, tuning_y, trace=T, strata=tuning_y, mtryStart=mtryStart, ntreeTry = 50)
  print("Tuning is over!")
  randomForest(x, y, mtry=tune[which.min(tune[, 
      2]), 1], ntree=ntree, strata=y, importance=T, do.trace=F)
}
```

```{r}
fit.rf.150t <- customTuneRF(train, ntree=200, mtryStart=7)
```

```{r}
fit.rf.150t$err.rate %>%
  as.data.frame() %>%
  cbind(ntrees=seq(1,nrow(fit.rf.150t$err.rate))) %>%
  rename("Default" = "0", "FullyPaid" = "1", "Overall" = "OOB") %>%
  gather(Error, Value, -ntrees) %>%
  ggplot() +
  geom_line(aes(x=ntrees, y= Value, col=Error)) +
  scale_y_continuous(labels = scales::percent) +
  scale_color_manual(values = c("darkred", "darkgreen", "black")) + 
  labs(x = "# Trees", y = "OOB Error", title = "Random Forest Progression") + 
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, size = 16))
```

Si osserva come il Random Forest minimizzi l'errore di previsione con l'utilizzo di 7 regressori (campionati causalmente per ogni albero), tuttavia l'errore con soli 4 regressori è estremamente simile, si decide quindi di utilizzarne solo 4 per il principio di parsimonia. Il sottocampionamento dell'insieme dei predittori permette di ottenere alberi meno correlati tra loro, con l'effetto di contenere così la varianza e quindi l'errore di previsione
Il numero di alberi utilizzati è pari a 200 ma dal secondo plot si osserva come già oltre i 150 alberi l'errore si stabilizzi.

```{r}
fit.rf.150t$importance %>%
  cbind("Variable"=rownames(fit.rf.150t$importance)) %>%
  as.data.frame() %>%
  mutate(MeanDecreaseAccuracy = as.numeric(as.character(MeanDecreaseAccuracy))) %>%
  ggplot() +
  geom_point(aes(x=MeanDecreaseAccuracy, y = reorder(Variable, MeanDecreaseAccuracy), col = MeanDecreaseAccuracy > 0)) +
  scale_color_manual(values=c("red", "black"), guide="none") +
  scale_x_continuous(labels=scales::percent) +
  labs(x= "Mean Decrease Accuracy", y = "", title = "Variables Importance in Random Forest", subtitle = paste("Overall Training Accuracy =", 1-misClassError(fit.rf.150t$y, as.numeric(as.character(fit.rf.150t$predicted)),0.5) )) + 
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, size = 16),
        plot.subtitle = element_text(hjust=0.5,size=12))
```

Si osserva come ci siano alcune variabili che danno un contributo nettamente più importante rispetto alle altre: 
*int_rate*, *term*, *loan_amnt*, *dti*; tuttavia si osserva come tutte le variabili diano un contributo positivo all'accuracy del modello.

```{r}
yhat.rf = predict(fit.rf.150t, test %>% select(-y), type="prob", norm.votes=T)

MyConfusionMatrix(test$y, yhat.rf[,2], threshold = 0.5, title="Random Forest")

cat("Accuracy del Random Forest:", 1-misClassError(test$y, yhat.rf[,2], threshold = 0.5))
```

```{r}
InformationValue::plotROC(test$y, yhat.rf[,2])
```

Il valore dell'AUC ci indica che siamo in presenza del miglior modello fin'ora ottenuto in termini di performance.

## Modello di Cox

L'ultimo modello presentato è il Modello di Regressione di Cox, appartenente alla classe dei Modelli di Sopravvivenza. Esso nasce nel contesto della Survival Analysis, in cui si studiano gli effetti di trattamenti medici o di fattori di rischio specifici. Questo tipo di analisi ricerca una stima della *"Sopravvivenza"* dei pazienti in un determinato periodo di osservazione (la parola *"Sopravvivenza"* non assume necessariamente un significato letterale ma a seconda del contesto indica l'assenza di uno specifico evento avverso, ad esemoio se si studia il beneficio dell'impianto di una protesi l'evento avverso può essere la frattura dell'osso). \
Per poter applicare i metodi della Survival Analysis e in particolare il Modello di Cox occorre fare le seguenti osservazioni:
- I prestiti sono interpretati come dei pazienti;
- Il Default del prestito è l'evento avverso (classe positiva);
- Non sono presenti dati censurati;
- L'evento di Early Payment può essere considerato come rischio competitivo all'evento di Default;
- È presente per costruzione un time-point fisso, ovvero il termine del prestito, oltre il quale non si hanno più dati. Fanno eccezione i prestiti ripagati in ritardo che complicano notevolmente l'analisi, per questi si può assumere che siano stati pagati al termine come semplificazione. Il tempo sarà espresso come percentuale del periodo del prestito per considerare in egual modo i prestiti con termini a 36 e 60 mesi.

Detta $f(t)$ la densità di eventi al tempo $t$, si definiscono:

- *Sopravvivenza* (o *Survival*) $S(t)$ : la probabilità che l'evento avvenga dopo il tempo t, in altre parole la probabilità che l'evento non si sia ancora verificato fino al tempo t; in formule: 
$S(t)=Pr(T>t)=\int_{t}^{\infty}{f(u) \space du}$ \
- *hazard ratio* $\lambda(t)$ : la "velocità" di evento al tempo T, dato che l'evento non si è verificato fino al tempo T; in formule:
$\lambda(t) = lim_{\Delta t \to 0} \frac{Pr(t<T<t+\Delta t|t>T)}{\Delta t}$.\

Il Modello di Cox assume la seguente forma funzionale per la sopravvivenza:\
$S_X(t)=S_0(t)e^{\beta X}$ \
dove $S_X(t)$ è la Sopravvivenza del prestito con il set di covariate $X$ al tempo $t$; $S_0(t)$ è la Sopravvivenza del prestito *"baseline"* (di riferimento) al tempo $t$, ovvero il prestito con tutte le covariate numeriche nulle e con una certo livello scelto delle variabili fattoriali; $\beta$ è il vettore dei coefficienti delle covariate $X$.\
Si osserva come il termine temporale compaia solo nella Sopravvivenza baseline, mentre l'effetto delle covariate è costante nel tempo. Questa scelta semplifica notevolmente il modello ma necessità della validità dell'ipotesi di *"Proportional Hazards" (PH)*, ovvero che l'effetto delle covariate sia costante nel tempo.

La grande differenza di questo modello rispetto ai precedenti è che viene presa in considerazione la variabile temporale, infatti non solo il modello è in grado di stimare la probabilità di Default di un prestito ma è anche in grado di esprimerla nel tempo ($1-S_X(t)$). 

Come detto l'evento di Early Payment può essere visto come un rischio competitivo all'evento di Default, tuttavia si decide di semplificare l'analisi trascurando questo tipo di evento e considerando tali prestiti come pagati al termine.

```{r, message=F}
prep_surv <- function(d, plots=T){
  d <- d %>% 
    as.data.frame() %>%
    mutate(actual_duration_to_term = as.numeric(last_pymnt_d - issue_d) / (as.numeric(substring(as.character(term),0,2))*(365/12)))
  if (plots){
    ht <- ggplot(d) + 
      geom_histogram(aes(x=actual_duration_to_term, y = ..count../sum(nrow(d)), fill=y), bins=50) +
      scale_fill_manual(labels=c("Default", "Fully Paid"), values=c("darkred","darkgreen")) + 
      scale_x_continuous(labels = scales::percent) + 
      scale_y_continuous(labels = scales::percent) + 
      labs(x="Time to end of lending", y = "Percentage of lendings", title="Time to end of lending by outcome") + 
      guides(fill=guide_legend(title="Outcome"))+
      theme_minimal() + 
      theme(plot.title = element_text(hjust = 0.5, size = 16),
          plot.subtitle = element_text(hjust = 0.5, size = 12))
    d_pie <- d %>%
      mutate(y = as.factor(case_when(
        y == 0 ~ 1, ### Default
        (y == 1) & (actual_duration_to_term < 0.8) ~ -1, ### Early Payment
        T ~ 0 ### Regular Payment
      )))
    pie <- piechart(d_pie, "y") +
      scale_fill_manual(labels=c("Regular Payment", "Early Payment", "Default"), values=c("darkgreen",  "darkgray", "darkred"))
  }
  d <- d %>%
    mutate(actual_duration_to_term = ifelse(y==0, actual_duration_to_term, 1)) %>%
    mutate(actual_duration_to_term = ifelse(actual_duration_to_term > 1, 1, actual_duration_to_term)) %>%
      # ignoro gli early payment e i pagamenti in ritardo, settando il tempo a 1 per loro, intendendo che questi prestiti sono sopravvissuti fino alla fine.
    mutate(status = ifelse(y==0,1,0)) # status è l'opposto di y: status 0 è Fully Paid, status 1 è Default ("Fallimento" nel linguaggio della Survival Analysis).
  if (plots) return(list(ht=ht,pie=pie))
  else return(d)
}

set.seed(1000)
train_surv <- sample_frac(acc, 0.7)
test_surv <- anti_join(acc, train_surv)
set.seed(1000)
train_surv <- prep_surv(train_surv, plots=F)
test_surv <- prep_surv(test_surv, plots=F)

ret <- prep_surv(acc, plots=T)
ret$pie
ret$ht
```

Si vuole quindi modificare i tempi all'evento di tutti i prestiti Fully Paid ponendolo pari al 100%.

```{r}
 ggplot(train_surv) + 
      geom_histogram(aes(x=actual_duration_to_term, y = ..count../sum(nrow(train_surv)), fill=as.character(status)), bins=25) +
      scale_fill_manual(labels=c("Fully Paid", "Default"), values=c("darkgreen","darkred")) + 
      scale_x_continuous(labels = scales::percent) + 
      scale_y_continuous(labels = scales::percent) + 
      labs(x="Time to end of lending", y = "Percentage of lendings", title="Time to end of lending by outcome") + 
      guides(fill=guide_legend(title="Outcome"))+
      theme_minimal() + 
      theme(plot.title = element_text(hjust = 0.5, size = 16),
          plot.subtitle = element_text(hjust = 0.5, size = 12))
```

```{r}
train_surv <- train_surv %>% select(all_of(model_variables), actual_duration_to_term, status) %>% select(-y)
test_surv <- test_surv %>% select(all_of(model_variables), actual_duration_to_term, status) %>% select(-y)
surv_scaling <- preProcess(train_surv %>% select(-status, -actual_duration_to_term), method = c("center", "scale"))
train_surv <- predict(surv_scaling, train_surv)
test_surv <- predict(surv_scaling, test_surv)

train_surv <- train_surv %>% group_by(status) %>% slice_sample(n=min(table(train$y))) %>% ungroup()
```

Si ricava la Curva di Sopravvivenza con l'utilizzo dello stimatore di Kaplan-Meier.

```{r message=F, warning=F}
surv.cox <- survfit(Surv(actual_duration_to_term, status) ~ 1, data = train_surv)
survplot <- ggsurvplot(surv.cox, xlab = "Time to End of Lending", legend= "none", conf.int = T, ggtheme = theme_minimal())
survplot$plot
summary(surv.cox, times = 0.5)
```

La sopravvivenza finale è pari al 50% poichè il training set è stato bilanciato tramite UnderSampling. Si vede come la curva si appiattisca sul finale, il che lascia intendere come quasi tutti i prestiti giunti al 75% del tempo finiscano poi ad essere ripagati interamente.

### Valutazione assunzione «Proportional Hazards»

Per valutare l'assunzione di Proportional Hazards per le covariate si procede dapprima con un check visuale della log(-log(Survival)), che valuta l'assunzione di PH per ogni covariata indipendentemente dalle altre, e successivamente ci si serve dei residui Schoenfeld per valutare l'assunzione di PH tenendo conto anche dalle altre covariate (modello multivariato).

1. log[-log(SKM (t))] --- Graphical check

2. Schoenfeld residuals --- Graphical check

#### 1. log[-log(SKM (t))] --- Graphical check

Per l'assunzione di PH abbiamo: $S_X(t) = S_0(t) e^{(\beta' X)}$
che può essere riscritta come: $log(-log(S_X(t))) - log(-log(S_0(t))) = \beta' X$
ne segue che la differenza tra i due logaritmi è costante rispetto al tempo se è valida l'assunzione di PH. 

Questo tipo di verifica può essere fatto unicamente sulle variabili non continue, poiché il $log(-log(S_0(t)))$ deve essere confrontato con $log(-log(S_X(t)))$ per ciascun valore di X su tutto l'intervallo temporale.

```{r}
par(mfrow=c(2,2),mar=c(4,4,2,2))
for (variable in model_variables[model_variables %notin% c(model_num_variables,"y")]){
  survfit(as.formula(paste("Surv(actual_duration_to_term, status) ~", variable)), data=train_surv) %>% 
    plot(fun="cloglog", ylab="log(-log(Survival))", xlab="log(time_to_term)", main=paste("Check PH assumption of", variable))
}
```

L'assunzione di PH pare valere per tutte le variabili categoriche, ad eccezione di *fico_class*.

#### 2. Schoenfeld residuals --- Graphical check

I residui Schoenfeld sono definiti come: $r_j = x_j - E[x_j|R_j]$ \
dove $x_j$ è la covariata del prestito che *fallisce* al tempo $t_{(j)}$ e $R_j$ è il risk set al tempo $t_{(j)}$.
Ci si aspetta che il valore atteso dei residui sia nullo, ne segue che l'andamento dei residui rispetto al tempo deve quantomeno essere costante. Si plottano i residui Schoenfeld standardizzati.

```{r}
model.cox.base <- coxph(Surv(actual_duration_to_term, status) ~ ., data = train_surv)
czph <- cox.zph(model.cox.base, terms = F)

par(mfrow=c(3,3),mar=c(4,4,2,2))
plot(czph, resid=FALSE)
```

Per molte variabili non vale l'assunzione di PH, le variabili numeriche vengono così escluse mentre per le variabili categoriche si crea un modello stratificato.

```{r}
train_ph <- train_surv %>% select(-application_type, -initial_list_status, -int_rate, -num_rev_tl_bal_gt_0, -pct_tl_nvr_dlq, -purpose, -revol_bal, -revol_util, -term, -total_acc, -verification_status, -mo_sin_earliest_cr_line, -num_bc_not_sats, -home_ownership, -addr_state)
```

```{r}
model.cox.ph <- coxph(Surv(actual_duration_to_term, status) ~ ., train_ph, x=T,y=T)
formula.cox.ph <- paste(names(model.cox.ph$coefficient), collapse = "+")
  
model.cox.strata <- coxph(
  Surv(actual_duration_to_term, status) ~
    dti +
    delinq_2yrs +
    emp_length +
    loan_amnt +
    mo_sin_old_il_acct +
    mort_acc+num_bc_tl +
    num_op_rev_tl +
    open_acc +
    percent_bc_gt_75 +
    pub_rec +
    tax_liens +
    fico_class +
    num_not_sats + 
    survival::strata(application_type) +
    survival::strata(initial_list_status) +
    survival::strata(purpose) +
    survival::strata(term) +
    survival::strata(verification_status) +
    survival::strata(home_ownership) +
    survival::strata(addr_state),
  train_surv, x =T, y=T) 
```

### Predizioni

```{r}
yhat.cox.strata <- c()
for (i in (seq(1,nrow(test_surv),ceiling(nrow(test_surv)/10))-1)){
  fit.cox.strata<-survfit(model.cox.strata,newdata=test_surv[(i+1):(i+ceiling(nrow(test_surv)/10)),])
  yhat <- as.numeric(summary(fit.cox.strata,times=1)$surv)
  yhat.cox.strata <- c(yhat.cox.strata,yhat)
  print(paste0("Processed ", i/nrow(test_surv)*100, "%"))
}

MyConfusionMatrix(as.factor(as.character(1-test_surv$status)), yhat.cox.strata, threshold = 0.5, title="Modello di Cox")

cat("Accuracy del Modello di Cox:", 1-misClassError((1-test_surv$status), yhat.cox.strata)
)

InformationValue::plotROC(1-test_surv$status, yhat.cox.strata)
```

Il modello è il peggiore tra quelli testati, ma ciò non stupisce poichè è stata tenuta fuori la variabile *int_rate* risultata finora la più significativa. Tale variabile, insieme alle altre variabili numeriche per cui non vale l'assunzione di PH, può essere reinserita nel modello con un termine di interazione con la variabile temporale, cosa che viene lasciata a una possibile futura implementazione.

# Risultati e Conclusioni

I modelli ottenuti hanno dimostrato tutti una discreta performance, che è stata finora valutata in termini di AUC e di Precision alla threshold di 0.5. \
I modelli possono essere visti come dei Recommender Systems, ovvero sistemi che offrono raccomandazioni su quali prestiti scegliere per investire. La metrica che viene utilizzata per la valutazione finale è di conseguenza la metrica tipica di questo tipo di sistemi, ovvero la Precision\@K. Avendo l'utente scelto un valore K di investimenti su cui puntare, il modello ritorna i K prestiti la cui probabilità di essere ripagati è massima, di questi K investimenti si valuta la Precision ovvero il numero di quelli che vengono effettivamente ripagati. 

```{r message=F, warning=F}
MyCalibrationPlot(test$y, list("Logit UnderSampling"=yhat.glm.under, "Logit PCA"=yhat.pca, "Logit Backward Elimination"=yhat.glm.backward, "Logit Ridge"=unname(yhat.ridge), "Logit Lasso"=unname(yhat.lasso),"Random Forest"=yhat.rf[,2], "Cox"= yhat.cox.strata))
```

Si vede come tutti i modelli ottenuti sottostimino le probabilità per i prestiti di essere ripagati, in altri termini abbiamo ottenuto modelli pessimistici. Tuttavia ciò non ci stupisce affatto, infatti tale comportamento è frutto del bilanciamento del training set che predilige l'accuratezza sulla classe *Default* piuttosto che ottenere predizioni complessivamente accurate. \
Occorre quindi considerare come i valori predetti dai modelli non possano essere correttamente interpretati come delle probabilità, ma semplicemente come una certa misura della confidenza del modello nel generare la predizione.

```{r fig.height=6, fig.width=10}
plot_precision_at_k(test$y, list("Logit UnderSampling"=yhat.glm.under, "Logit PCA"=yhat.pca, "Logit Backward Elimination"=yhat.glm.backward, "Logit Ridge"=unname(yhat.ridge), "Logit Lasso"=unname(yhat.lasso),"Random Forest"=yhat.rf[,2], "Cox"=yhat.cox.strata))
```

In questo plot l'asse x superiore riporta il valore del K aspettato, ovvero il valore corrispondente alla percentuale del test set nel caso in cui si avessero 40,000 prestiti tra cui scegliere. Il valore di 40,000 è stato scelto avendo osservato il plot nella sezione 'Numero di osservazioni / tempo' e avendo notato come negli ultimi anni vengono accettati circa 40,000 prestiti al mese, poichè tale valore è in crescita (~lineare) da più di 6 anni ci si aspetta di trovare almeno questo numero di prestiti ogni mese su cui investire.

Si osserva come il Modello di Cox e la Regressione Logistica con PCA conducono alle previsioni peggiori, mentre gli altri modelli hanno tutti performance molto simili e il migliore dipende dal valore di K scelto, tale comportamento ci induce a scegliere il modello più parsimonioso e più interpretabile. La richiesta di interpretabilità sicuramente porta ad escludere il Random Forest, poichè questo si comporta come una *black box*, mentre gli altri modelli di regressione logistica hanno tutti lo stesso grado di interpretabilità. Valutando questi modelli in grado di parsimonia, il migliore risulta essere la Regressione Logistica con Penalizzazione Lasso, poichè non solo comprime i coefficienti (come la Ridge) ma ne seleziona anche un sottoinsieme, escludendone ben 8 (contro i 7 della Backward Selection), qualificandosi così come il modello più parsimonioso.

Supponendo quindi di voler fare 100 investimenti la precision aspettata è pari al 98%, ci si aspetta quindi di suggire 98 prestiti che saranno restituiti e 2 prestiti che non saranno restituiti. Da tenere a mente come il modello random avrebbe avuto una Precision\@K costante su K e pari alla prevalenza della classe 'Fully Paid', ovvero il 79.2%.

# Sviluppi Futuri

Il modello infine ottenuto porta risultati eccellenti per valori di K bassi e che rimangono molto buoni anche per valori di k più elevati. Il modello è, nel complesso, in grado di suggerire con alta confidenza quali prestiti siano i migliori per investire, tuttavia questo suggerimento tiene di conto unicamente della probabilità che il prestito sia ripagato ma non tiene altresì conto del tasso di interesse sul prestito ovvero del Return On Investment (ROI). Per rispondere completamente alle esigenze del cliente si potrà aggiungere questa informazione al modello e considerare piuttosto che la probabilità del prestito di essere ripagato, il ROI aspettato ($probabilità \times ROI$). Infine sarà così possibile definire delle strategie di investimento su misura per i clienti.

Per il futuro è inoltre interessante approfondire il modello di Cox, risultato in questa prima fase il modello peggiore ma considerato quello con più potenzialità, infatti questo a differenza degli altri modelli tiene conto del variabile temporale ed è in grado di offrire previsioni legate proprio al tempo. Ciò offre la possibilità di poter modellare anche gli *"Early Payment"* e tenere inoltre di conto che non tutti gli investimenti che vanno in *"Default"* sono da evitare, si potrebbero infatti scegliere investimenti con tasso di interesse elevato e che potenzialmente andranno in Default ma solo dopo un periodo sufficiente ad aver ottenuto i ricavi desiderati. 
