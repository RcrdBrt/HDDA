---
title: "Progetto HDDA AA2021/2022"
output: html_notebook
---

# Libraries

```{r message=FALSE, warning=FALSE, paged.print=FALSE, include=F}
if(!require(data.table)) install.packages("data.table")
if(!require(Hmisc)) install.packages("Hmisc")
if(!require(dplyr)) install.packages("dplyr")
if(!require(tidyr)) install.packages("tidyr")
if(!require(corrplot)) install.packages("corrplot")
if(!require(lubridate)) install.packages("lubridate")
if(!require(stringr)) install.packages("stringr")
if(!require(sampling)) install.packages("sampling")
if(!require(leaps)) install.packages("leaps")
if(!require(bestglm)) install.packages("bestglm")
if(!require(glmnet)) install.packages("glmnet")
if(!require(factoextra)) install.packages("factoextra")
if(!require(caret)) install.packages("caret")
if(!require(ranger)) install.packages("ranger")
if(!require(randomForest)) install.packages("randomForest")
if(!require(InformationValue)) install.packages("InformationValue")
if(!require(kernlab)) install.packages("kernlab")
if(!require(e1071)) install.packages("e1071")
if(!require(survival)) install.packages("survival")
if(!require(survminer)) install_github("kassambara/survminer", build_vignettes = FALSE)
if(!require(riskRegression)) install.packages("riskRegression")
if(!require(parallel)) install.packages("doParallel")
if(!require(doMC)) install.packages("doMC")
if(!require(pander)) install.packages("pander")
if(!require(patchwork)) install.packages("patchwork")

library(data.table)
library(Hmisc)
library(tidyr)
library(dplyr)
library(corrplot)
library(lubridate)
library(stringr)
library(sampling)
library(leaps)
library(bestglm)
library(glmnet)
library(factoextra)
library(caret)
library(ranger)
library(randomForest) 
library(InformationValue) 
library(kernlab) 
library(e1071) 
library(survival)
library(riskRegression)
library(parallel)
library(doMC)
library(pander)
library(patchwork)
library(car)

`%notin%` <- Negate(`%in%`)

registerDoMC(cores = detectCores())
#cl = makePSOCKcluster(detectCores())
#registerDoParallel(cl)
```

# Useful functions

```{r, include=F}
piechart <- function(d, group, legend_title = "Outcome"){
  piepercent <- d %>% 
    group_by((!!sym(group))) %>% 
    summarise(n_group = n()) %>% 
    mutate(n_perc= round( n_group/nrow(d)* 100,2))
  
  ggplot(piepercent, aes(x = "", y = n_perc, fill = !!sym(group))) +
    geom_bar(width = 1, stat = "identity", color = "white") +
    coord_polar("y", start = 0) +
    geom_text(aes(label = n_perc), position = position_stack(vjust = 0.5),color = "white")+
    guides(fill=guide_legend(title=legend_title))+
    theme_void()
}


MyConfusionMatrix <- function(actuals, predictions, threshold=0.5, title=""){
  cm <- caret::confusionMatrix(as.factor(ifelse(predictions>threshold,1,0)), actuals, threshold, dnn=c("Prediction", "Actual"), positive="1", mode="prec_recall")
  ggplot(as.data.frame(cm$table), aes(Actual, Prediction, fill= Freq)) +
    geom_tile() + geom_text(aes(label=Freq), fontface="bold") +
    scale_fill_distiller(palette="Greens",guide="none", direction = 1) +
    labs(x = "Actual",y = "Prediction", title=title, subtitle=
           paste(
             "Precision at 0.5 threshold:",
             round(precision(actuals,predictions,threshold),3)*100, "%")) +
    scale_x_discrete(labels=c("Default", "Fully Paid")) +
    scale_y_discrete(labels=c("Fully Paid","Default"), limits=rev) +
    theme_minimal() + 
    theme(plot.title = element_text(hjust = 0.5, size = 16),
          plot.subtitle = element_text(hjust = 0.5, size = 12)) 
}


plot_precision_vs_npred <- function(actuals, predictions, title=""){
  p <- vector("list", length(predictions))
  names(p) <- names(predictions)
  n <- vector("list", length(predictions))
  names(n) <- names(predictions)
  for (i in 1:length(predictions)){
      for (threshold in seq(0,1,0.05)){
        p[[i]] <- c(p[[i]],precision(actuals, predictions[[i]], threshold = threshold))
        n[[i]] <- c(n[[i]],length(predictions[[i]][predictions[[i]] >= threshold]))
      }
  }
  
  res <- as.data.frame(list(p=p,n=n))
  res$i <- 1:nrow(res)
  res <- gather(res,var,val,-i) %>% separate(var, c("variable", "Model")) %>% reshape(idvar = c("i","Model"), timevar = "variable", direction = "wide")
  names(res)[c(3,4)] <- c("p","n")
  
  plotly::plot_ly(res, x=~p*100, y = ~n/length(actuals)*100, type="scatter", mode = "lines+markers", color=~Model, opacity = 1) %>% 
  plotly::layout(title = "Results",
                 xaxis = list(title = 'Precision',  ticksuffix = "%"),
                 yaxis = list(title = 'Number of predicted Fully Paid', ticksuffix = "%"),
                 legend = list(x=0.8,y=0.9))

}
```

# Data Ingestion

```{r message=FALSE, warning=FALSE, cache=TRUE, paged.print=FALSE}
acc_full <- fread("archive/accepted_2007_to_2018Q4.csv", sep = ",", header = T)
```

```{r, cache=TRUE}
variables <- c(
  "addr_state",
  "annual_inc",
  "application_type",
  "delinq_2yrs",
  "earliest_cr_line",
  "emp_length",
  #"emp_title", # troppi livelli, circa 600k
  "fico_range_high",
  "fico_range_low",
  "grade",
  "home_ownership",
  "initial_list_status",
  "int_rate",
  "issue_d",
  "last_pymnt_d",
  "loan_amnt",
  "funded_amnt",
  "loan_status",
  "mo_sin_old_il_acct",
  "mo_sin_old_rev_tl_op",
  "mort_acc",
  "num_bc_sats",
  "num_bc_tl",
  "num_op_rev_tl",
  "num_rev_accts",
  "num_rev_tl_bal_gt_0",
  "num_sats",
  "open_acc",
  "pct_tl_nvr_dlq",
  "percent_bc_gt_75",
  "pub_rec",
  "pub_rec_bankruptcies",
  "purpose",
  "revol_bal",
  "revol_util",
  "sub_grade",
  "tax_liens",
  "term",
  "total_acc",
  #"total_cu_tl",
  "verification_status"
)
```

```{r}
acc <- acc_full
acc <- acc[,..variables]
acc <- acc[complete.cases(acc),]
acc <- acc[!apply(acc, 1, function(x) any(x=="")),]
```

```{r}
fwrite(acc, "archive/accepted_clean.csv", sep = ",")
```

# Data Preparation
```{r}
acc <- fread("archive/accepted_clean.csv", sep=",",header=T)
acc <- as.data.frame(acc)
numeric_variables <- c(
  "annual_inc",
  "delinq_2yrs",
  "mo_sin_earliest_cr_line",
  "fico_range_high",
  "fico_range_low",
  "int_rate",
  "loan_amnt",
  "funded_amnt",
  "mo_sin_old_il_acct",
  "mo_sin_old_rev_tl_op",
  "mort_acc",
  "num_bc_sats",
  "num_bc_tl",
  "num_op_rev_tl",
  "num_rev_accts",
  "num_rev_tl_bal_gt_0",
  "num_sats",
  "open_acc",
  "pct_tl_nvr_dlq",
  "percent_bc_gt_75",
  "pub_rec",
  "pub_rec_bankruptcies",
  "revol_bal",
  "revol_util",
  "tax_liens",
  "total_acc"
)

acc <- acc %>%
  mutate(
    mo_sin_earliest_cr_line=as.integer(round((as.Date("2018-12-01")-lubridate::my(earliest_cr_line))/(365.25/12))),
    issue_d = lubridate::my(issue_d),
    last_pymnt_d = lubridate::my(last_pymnt_d),
    emp_length=as.numeric(str_extract(emp_length, "[0-9]+"))
  ) %>%
  mutate(
    emp_length = case_when(
      emp_length < 5 ~ "low",
      emp_length < 10 ~ "mid",
      T ~"high")) %>%
  select(-earliest_cr_line)

acc_with_current <- acc

acc <- acc %>% 
  filter(loan_status %notin% c("Current", "In Grace Period", "Late (16-30 days)")) %>%
  mutate(y=case_when(
    loan_status == "Late (31-120 days)" ~ 0,
    loan_status == "Charged Off" ~ 0,
    loan_status == "Default" ~ 0,
    loan_status == "Fully Paid" ~ 1
  )) %>%
  mutate(addr_state=case_when(
    addr_state %in% c("WA", "OR", "CA", "HI", "AL", "AK") ~ "West Coast",
    addr_state %in% c("MT", "ID", "NV", "UT", "CO", "WY") ~ "The Rocky Mountains",
    addr_state %in% c("AZ", "NM", "TX", "OK") ~ "South West",
    addr_state %in% c("ND", "SD", "NE", "KS", "MN", "IA", "MO", "WI", "IL", "IN", "MI", "OH") ~ "Midwest",
    addr_state %in% c("AR", "LA", "MS", "KY", "TN", "AL", "GA", "FL", "NC", "SC", "VA", "WV") ~ "South East",
    addr_state %in% c("ME", "VT", "NH", "MA", "RI", "CT") ~ "New England",
    addr_state %in% c("NY", "PA", "NJ", "MD", "DE", "DC") ~ "Mid Atlantic",
  )) %>%
  # livelli erano molto sbilanciati, optato per 3 livelli
  mutate(purpose=case_when(
    purpose %in% c("debt_consolidation") ~ "debt_consolidation",
    purpose %in% c("credit_card") ~ "credit_card",
    T ~ "other"
  )) %>% 
  mutate(
    addr_state=as.factor(addr_state),
    y=as.factor(y),
    application_type=as.factor(application_type),
    emp_length=as.factor(emp_length),
    grade=as.factor(grade),
    home_ownership=as.factor(home_ownership),
    initial_list_status=as.factor(initial_list_status),
    purpose=as.factor(purpose),
    sub_grade=as.factor(sub_grade),
    term=as.factor(term),
    verification_status=as.factor(verification_status)
  ) %>% 
  filter(home_ownership %notin% c("ANY", "NONE", "OTHER"))  %>%
  mutate(home_ownership = as.factor(as.character(home_ownership))) %>% # remove empty levels
  select(-loan_status)
set.seed(1000)
source("data_preparation/outliers.R", print.eval = T)
```

Possiamo pensare di utilizzare anche i Current tipo con 90% già ripagato. Ma ci servono? No!

# Data Exploration

### numero di osservazioni / tempo

```{r, fig.width=10, fig.height=6}
source("data_preparation/plot_osservazioni_e_tempo.R", print.eval = T)
```

### Pie chart default vs fully paid

```{r}
piechart(acc,group="y") +
  scale_fill_manual(labels=c("Default", "Fully Paid"), values=c("darkred","darkgreen"))
```

### ROI boxplot/violinplot (for each grade?)

```{r}
source("data_preparation/ROI_violin.R", print.eval = T)
```


### Violin plot for each numeric variable and barplot for categorical variables

```{r, fig.width=10, fig.height=6}
source("data_preparation/all_violin.R", print.eval = T)
```


### Investigate correlations and fix


```{r}
corrplot(cor(acc[,numeric_variables]))
```

```{r}
cor.mtest <- function(mat, ...) {
    mat <- as.matrix(mat)
    n <- ncol(mat)
    p.mat<- matrix(NA, n, n)
    diag(p.mat) <- 0
    for (i in 1:(n - 1)) {
        for (j in (i + 1):n) {
            tmp <- cor.test(mat[, i], mat[, j], ...)
            p.mat[i, j] <- p.mat[j, i] <- tmp$p.value
        }
    }
  colnames(p.mat) <- rownames(p.mat) <- colnames(mat)
  p.mat
}
# matrix of the p-value of the correlation
chart_pic_file = tempfile(pattern="file", tmpdir=tempdir(), fileext = ".png")
p.mat <- cor.mtest(acc[,numeric_variables])
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
png(filename = chart_pic_file, width = 900, height = 900, type = "cairo")
corrplot::corrplot(cor(acc[,numeric_variables]), method="color", col=col(200),  
         type="upper", 
         addCoef.col = "black", # Add coefficient of correlation
         tl.col="black", tl.srt=45, tl.cex = .7, number.cex=0.75,  #Text label color and rotation
         # Combine with significance
         p.mat = p.mat, sig.level = 0.2, insig = "blank", 
         # hide correlation coefficient on the principal diagonal
         diag=T 
         )
```

```{r pressure, out.width= '100%'}
knitr::include_graphics(chart_pic_file)
```

```{r}
ids <- sample(c(1:nrow(acc)), 8e4)
plot(acc[ids,"mo_sin_earliest_cr_line"], acc[ids,"mo_sin_old_rev_tl_op"])
```

```{r}
plot(acc[ids, "num_sats"], acc[ids, "open_acc"])
```

# Fix Correlation

```{r}
### categorizzazione fico_mean (https://www.investopedia.com/terms/f/ficoscore.asp)
acc <- acc %>% 
  mutate(fico_mean = (fico_range_high+fico_range_low)/2) %>%
  select(-fico_range_high, -fico_range_low) %>% 
  mutate(fico_class=case_when(
    fico_mean <= 580 ~ "Poor",
    fico_mean > 580 & fico_mean <= 669 ~ "Fair",
    fico_mean > 670 & fico_mean <= 739 ~ "Good",
    fico_mean > 740 & fico_mean <= 799 ~ "Very Good",
    fico_mean > 800 ~ "Exceptional"
  )) %>% select(-fico_mean) %>% mutate(fico_class=as.factor(fico_class))

acc <- acc %>% 
  mutate(num_not_sats = (open_acc - num_sats)/open_acc,
         num_bc_not_sats = (num_bc_tl - num_bc_sats)/num_bc_tl) %>%
  select(-num_sats, num_bc_sats)

# tolte le correlazioni, restano queste variabili esplicative numeriche
new_numeric_variables <- c(
  "annual_inc",
  "delinq_2yrs",
  "mo_sin_earliest_cr_line",
  "int_rate",
  "loan_amnt",
  "mo_sin_old_il_acct",
  "mo_sin_old_rev_tl_op",
  "mort_acc",
  "num_bc_not_sats",
  "num_bc_tl",
  "num_op_rev_tl",
  "num_rev_accts",
  "num_rev_tl_bal_gt_0",
  "num_not_sats",
  "open_acc",
  "pct_tl_nvr_dlq",
  "percent_bc_gt_75",
  "pub_rec",
  "pub_rec_bankruptcies",
  "revol_bal",
  "revol_util",
  "tax_liens",
  "total_acc"
)
corrplot(cor(acc[, new_numeric_variables]))
```


# Modeling

```{r}
model_variables <-  names(acc)[names(acc) %notin% c(
  "grade",
  "funded_amnt",
  "issue_d",
  "last_pymnt_d",
  "num_sats",
  "num_bc_sats",
  "sub_grade",
  "pub_rec_bankruptcies",
  "num_rev_accts",
  "mo_sin_old_rev_tl_op"
)]

model_num_variables <- model_variables[model_variables %in% new_numeric_variables]

model_acc <- acc[,model_variables] %>% relocate(y, .after = last_col())

model_acc[, model_num_variables] <- scale(model_acc[, model_num_variables])

corrplot(cor(model_acc[, model_num_variables]))
```

```{r message=F}
set.seed(1000)
train <- sample_frac(model_acc, 0.7)
test <- anti_join(model_acc,train)
```

To deal with class imbalance we define weights so that the model's predictions will be fair to both classes.

```{r}
model_weights <- ifelse(train$y == "0",
                        (1/table(train$y)[1]) * 0.5, ### (1 / numero osservazioni classe 0) / 2
                        (1/table(train$y)[2]) * 0.5) ### (1 / numero osservazioni classe 1) / 2
# sum(model_weights) == 1 TRUE
```

## Logistic Regression

### Plain

```{r}
fit.glm.unbalanced <- glm(y~., train, family="binomial")
vif(fit.glm.unbalanced)
```


```{r}
yhat.glm.unbalanced <- predict(fit.glm.unbalanced, test, type="response")
cat("Train Accuracy of Unbalanced Model:", 1-InformationValue::misClassError(train$y, fit.glm.unbalanced$fitted.values), "\n")
cat("Test Accuracy of Unbalanced Model:", 1-InformationValue::misClassError(test$y, yhat.glm.unbalanced))
```

```{r}
train.under <- train %>% group_by(y) %>% slice_sample(n=min(table(train$y))) %>% ungroup()
fit.glm.under <- glm(y~., train.under, family="binomial")
yhat.glm.under<- predict(fit.glm.under, test, type="response")
cat("Train Accuracy of UnderSampling Model:", 1-InformationValue::misClassError(train.under$y, fit.glm.under$fitted.values), "\n")
cat("Test Accuracy of UnderSampling Model:", 1-InformationValue::misClassError(test$y, yhat.glm.under))
```

```{r}
fit.glm <- glm(y~., train, family="binomial", weights=model_weights)
yhat.glm <- predict(fit.glm, test, type="response")
cat("Train Accuracy of Balanced Model:", 1-InformationValue::misClassError(train$y, fit.glm$fitted.values), "\n")
cat("Test Accuracy of Balanced Model:", 1-InformationValue::misClassError(test$y, yhat.glm))
```


```{r fig.width=10, fig.height=4}
p1 <- MyConfusionMatrix(train$y, test$y, fit.glm.unbalanced$fitted.values, yhat.glm.unbalanced, title="Logit Unbalanced")
p2 <- MyConfusionMatrix(train.under$y, test$y, fit.glm.under$fitted.values, yhat.glm.under, title="Logit UnderSampling")
p3 <- MyConfusionMatrix(train$y, test$y, fit.glm$fitted.values, yhat.glm, title="Logit Weighted")
p1+p2+p3
```

L'utilizzo dei pesi ci permette di avere previsioni più bilanciate, al costo di peggiorare l'accuracy totale del nostro modello si riescono ad ottenere previsioni più attendibili sulla classe minoritaria (Default). Si osserva infatti come il modello senza il bilanciamento ha un'accuracy sulla classe Default del 9%, mentre il modello bilanciato arriva al 63%. In conclusione otteniamo una Precision sulla classe Fully Paid più elevata.
Da qui in avanti verranno prodotti solo modelli bilanciati tramite l'utilizzo dei pesi.

```{r}
InformationValue::plotROC(test$y, yhat.glm.under)
```

Per avere il modello il più parsimonioso possibile, viene selezionato il training set con undersampling in quanto quello pesato, con un numero di osservazioni di gran lunga superiore, non apporta un apprezzabile incremento di precisione.

```{r}
train <- train.under
```

### PCA

```{r}
pca.res <- prcomp(train[, model_num_variables])

plot(x=1:length(pca.res$sdev), y=pca.res$sdev, type="p", col=ifelse(pca.res$sdev>=1,"red","blue") , xlab="# components", ylab="Standard Deviation") # si prendono quelle > 1
abline(h=1, col="red")
fviz_eig(pca.res, chioce="eigenvalue")
```


```{r}
train.pca <- as.data.frame(as.matrix(train[,model_num_variables]) %*% pca.res$rotation)[,1:7] # prime sette variabili
test.pca <- as.data.frame(as.matrix(test[,model_num_variables]) %*% pca.res$rotation)[,1:7] # prime sette variabili

train.pca$y <- train$y
test.pca$y <- test$y
```

```{r}
fit.pca <- glm(y~., train.pca, family="binomial")

yhat.pca <- predict(fit.pca, test.pca, type="response")

MyConfusionMatrix(train.pca$y, test.pca$y, fit.pca$fitted.values, yhat.pca, title = "Logit - PCA")
cat("Train Accuracy = ", 1-InformationValue::misClassError(train.pca$y, fit.pca$fitted.values), "\n")
cat("Test Accuracy = ", 1-InformationValue::misClassError(test.pca$y, yhat.pca))
```

```{r}
InformationValue::plotROC(test.pca$y, yhat.pca)
```

Cerchiamo quindi altri metodi blah blah.

### Backward Elimination

```{r}
train.tiny <- train %>% slice_sample(n=5e4) %>% ungroup()

fit.glm.tiny <- glm(y~., train.tiny, family="binomial")

glm.backward = step(fit.glm.tiny, direction = "backward", trace=T, k=2)

fit.glm.backward <- glm(glm.backward$formula, train, family = "binomial")

yhat.glm.backward = predict(fit.glm.backward, test, type="response")

MyConfusionMatrix(test$y, yhat.glm.backward, title = "Logit - Backward Elimination")

cat("Train Accuracy = ", 1-InformationValue::misClassError(train$y, fit.glm.backward$fitted.values), "\n")
cat("Test Accuracy = ", 1-InformationValue::misClassError(test$y, yhat.glm.backward))
```

```{r}
InformationValue::plotROC(test$y, yhat.glm.backward)
```

Backward e' troppo pesante, cerchiamo altri metodi

### Ridge

```{r}
X.train <- model.matrix(y~., train)[,-1]
K <- 10
fit.ridge.cv <- cv.glmnet(X.train, train$y, alpha=0, family="binomial", nfolds = K, grouped = FALSE, type.measure = "class", parallel = T)
plot(fit.ridge.cv)
```

Perchè abbiamo usato cv.glmnet??? Perchè vogliamo mostrare come la crossvalidation sia nel nostro caso piuttosto inutile, avendo un dataset molto grande i risultati sulle varie folds sono estremamente stabili/simili. Così abbiamo giustificato l'utilizzo del metodo holdout invece della cross-validation.

```{r}
X.test <- model.matrix(y~., test)[,-1]

yhat.ridge = predict(fit.ridge.cv, X.test, s = "lambda.1se",  type="response")

MyConfusionMatrix(test$y, yhat.ridge, threshold = 0.5, title = "Ridge Logit")
cat("Accuracy = ", 1-InformationValue::misClassError(test$y, yhat.ridge, threshold = 0.5))
```


```{r}
InformationValue::plotROC(test$y, yhat.ridge)
```

Commento.
Grafico confronto coefficienti.

### Lasso

```{r}
K <- 10
fit.lasso.cv <- cv.glmnet(X.train, train$y, alpha=1, family="binomial", nfolds = K, grouped = FALSE, type.measure = "class", parallel = T)
plot(fit.lasso.cv)
```

```{r}
cat(paste(c("Variables excluded by lasso selection are:", rownames(coef(fit.lasso.cv, s = "lambda.1se"))[(coef(fit.lasso.cv, s = "lambda.1se") == 0)[,1]]), collapse=" \n" ))
```

```{r}
yhat.lasso = predict(fit.lasso.cv, X.test, s = "lambda.1se",  type="response")

MyConfusionMatrix(test$y, yhat.lasso, threshold = 0.5, title="Lasso Logit")
cat("Accuracy = ", 1-InformationValue::misClassError(test$y, yhat.lasso, threshold = 0.5))
```

```{r}
InformationValue::plotROC(test$y, yhat.lasso)
```

La lasso e' bella!

# Random Forest

```{r}
customTuneRF<- function (data, ntree, model_weights, mtryStart) 
{
  data <- data %>% group_by(y) %>% slice_sample(n=1e5) %>% ungroup() 
  x <- data %>% select(-y)
  y <- data$y
  tuning_data <- data %>% group_by(y) %>% slice_sample(n=5e4) %>% ungroup()
  tuning_x <- tuning_data %>% select(-y)
  tuning_y <- tuning_data$y
  class_weights <- unique(model_weights)
  class_weights <- c("0" = max(class_weights), "1" = min(class_weights))
  tune <- tuneRF(tuning_x, tuning_y, trace=T, strata=tuning_y, classwt=class_weights, mtryStart=mtryStart)
  print("Tuning is over!")
  randomForest(x, y, mtry=tune[which.min(tune[, 
      2]), 1], ntree=ntree, strata=y, classwt=class_weights, importance=T, do.trace=T)
}
```

```{r}
fit.rf.150t = customTuneRF(train, ntree=150, model_weights, mtryStart=7)
```

```{r}
plot(fit.rf.150t, lty=c(1,2,3))
legend("topright", inset=.05,cex = 0.7, 
  c("OOB Mean","OOB Default","OOB Fully paid"), col=c("black", "red", "green"), lwd=2, lty=c(1,2,3), horiz=TRUE)
```

```{r}
varImpPlot(fit.rf.150t, type=1, scale=T)
#abline(v=0, col="red")
```

Controllare presenza variabili che peggiorano e in caso girare nuovo randomforest senza di esse.

```{r}
yhat.rf = predict(fit.rf.150t, test %>% select(-y), type="prob", norm.votes=T)

MyConfusionMatrix(test$y, yhat.rf[,2], threshold = 0.5)

cat("Accuracy = ", 1-InformationValue::misClassError(test$y, yhat.rf[,2], threshold = 0.5))
```

```{r}
InformationValue::plotROC(test$y, yhat.rf[,2])
```


# Survival Analysis

In order to perform survival analysis we define three possible outcomes for a lending: "Fully Paid", "Default" and "Pre-Paid", whose meanings are pretty clear.

To distinguish between Regalur Payment and Early Payment we define a tolerance of -1 month since our dates are all represented as month-year.

```{r}
acc_surv <- acc
acc_surv <- acc_surv %>% 
  mutate(actual_duration_to_term = as.numeric(last_pymnt_d - issue_d) / (as.numeric(substring(as.character(term),0,2))*30))
hist(as.numeric(acc_surv[acc$y==0, "actual_duration_to_term"]), col = "darkgreen")
hist(as.numeric(acc_surv[acc$y==1, "actual_duration_to_term"]), col="darkred",add =T)
acc_surv_pie <- acc_surv %>%
  mutate(y = as.factor(case_when(
      y == 1 ~ 1, ### Default
      (y == 0) & (actual_duration_to_term < 0.8) ~ -1, ### Early Payment
      T ~ 0 ### Regular Payment
    ))
  )
piechart(acc_surv_pie, "y") +
  scale_fill_manual(labels=c("Regular Payment", "Early Payment", "Default"), values=c("darkgreen", "darkgray", "darkred"))
```

## Survival function

```{r}
acc_surv <- acc_surv %>%
  mutate(actual_duration_to_term = ifelse(y==1, actual_duration_to_term, 1)) %>%
  mutate(actual_duration_to_term = ifelse(actual_duration_to_term > 1, 1, actual_duration_to_term)) %>%
  mutate(status = as.numeric(as.character(y)))
# ignoro gli early payment e i pagamenti in ritardo, settando il tempo a 1 per loro, intendendo che questi prestiti sono sopravvissuti fino alla fine.

hist(as.numeric(acc_surv[acc_surv$y==0, "actual_duration_to_term"]), col="darkgreen", breaks=seq(0,1,0.01))
hist(as.numeric(acc_surv[acc_surv$y==1, "actual_duration_to_term"]), col = "darkred", breaks=seq(0,1,0.05),add=T)

fit <- survfit(Surv(actual_duration_to_term, status) ~ 1, data = acc_surv)
ggsurvplot(fit, xlab = "Years", legend= "none", ggtheme = theme_minimal())
summary(fit, times = 0.5)
```

## Cox

```{r}
acc_surv <- acc_surv %>% select(all_of(model_variables), actual_duration_to_term, status) %>% select(-y)
acc_surv[, new_numeric_variables[new_numeric_variables %in% names(acc_surv)]] <- scale(model_acc[, new_numeric_variables[new_numeric_variables %in% names(acc_surv)]])
```

### Valutare assunzione «Proportional Hazards»

Per valutare l'assunzione di Proportional Hazards per le covariate si procede dapprima con un check visuale della log(-log(Survival)), che valuta l'assunzione di PH per ogni covariata indipendentemente dalle altre, e successivamente ci si serve dei residui Schoenfeld per valutare l'assunzione di PH tenendo conto anche dalle altre covariate (modello multivariato).

1. log[-log(SKM (t))] --- Graphical check

2. Schoenfeld residuals --- Graphical check

Infine si valuta la forma funzionale assunta dalle variabili continue *age* e *creat*, confrontandola con la forma lineare.


#### 1. log[-log(SKM (t))] --- Graphical check

Per l'assunzione di PH abbiamo: $S_X(t) = S_0(t) e^{(\beta' X)}$
che può essere riscritta come: $log(-log(S_X(t))) - log(-log(S_0(t))) = \beta' X$
ne segue che la differenza tra i due logaritmi è costante rispetto al tempo se è valida l'assunzione di PH. 

Questo tipo di verifica può essere fatto unicamente sulle variabili non continue, poiché il $log(-log(S_0(t)))$ deve essere confrontato con $log(-log(S_X(t)))$ per ciascun valore di X su tutto l'intervallo temporale.

```{r}
par(mfrow=c(2,2),mar=c(4,4,2,2))

km.addr_state <- survfit(Surv(actual_duration_to_term, status) ~ addr_state, data=acc_surv)
plot(km.addr_state, fun="cloglog", ylab="log(-log(Survival))", xlab="log(time_to_term)", main="Check PH assumption of address state")

km.appl_type <- survfit(Surv(actual_duration_to_term, status) ~ application_type, data=acc_surv)
plot(km.appl_type, fun="cloglog", ylab="log(-log(Survival))", xlab="log(time_to_term)", main="Check PH assumption of application type")

km.emp_length <- survfit(Surv(actual_duration_to_term, status) ~ emp_length, data=acc_surv)
plot(km.emp_length, fun="cloglog", ylab="log(-log(Survival))", xlab="log(time_to_term)", main="Check PH assumption of employment length")

km.home_ownership <- survfit(Surv(actual_duration_to_term, status) ~ home_ownership, data=acc_surv)
plot(km.addr_state, fun="cloglog", ylab="log(-log(Survival))", xlab="log(time_to_term)", main="Check PH assumption of home ownership")

km.init_list <- survfit(Surv(actual_duration_to_term, status) ~ initial_list_status, data=acc_surv)
plot(km.init_list, fun="cloglog", ylab="log(-log(Survival))", xlab="log(time_to_term)", main="Check PH assumption of initial list status")

km.purpose <- survfit(Surv(actual_duration_to_term, status) ~ purpose, data=acc_surv)
plot(km.purpose, fun="cloglog", ylab="log(-log(Survival))", xlab="log(time_to_term)", main="Check PH assumption of purpose")

km.term <- survfit(Surv(actual_duration_to_term, status) ~ term, data=acc_surv)
plot(km.term, fun="cloglog", ylab="log(-log(Survival))", xlab="log(time_to_term)", main="Check PH assumption of term")

km.ver_status <- survfit(Surv(actual_duration_to_term, status) ~ verification_status, data=acc_surv)
plot(km.ver_status, fun="cloglog", ylab="log(-log(Survival))", xlab="log(time_to_term)", main="Check PH assumption of verification status")

km.fico_class <- survfit(Surv(actual_duration_to_term, status) ~ fico_class, data=acc_surv)
plot(km.fico_class, fun="cloglog", ylab="log(-log(Survival))", xlab="log(time_to_term)", main="Check PH assumption of fico class")
```

L'assunzione di PH pare valere per tutte le variabili categoriche, mentre sembra che non valga per la "fico class".

#### 2. Schoenfeld residuals --- Graphical check

I residui Schoenfeld sono definiti come: $r_j = x_j - E[x_j|R_j]$ \
dove $x_j$ è la covariata dell'individuo che *fallisce* al tempo $t_{(j)}$ e $R_j$ è il risk set al tempo $t_{(j)}$.
Ci si aspetta che il valore atteso dei residui sia 0, ne segue che l'andamento dei residui rispetto al tempo deve essere costante. Si plottano i residui Schoenfeld standardizzati.


```{r}
model_base <- coxph(Surv(actual_duration_to_term, status) ~ ., data = acc_surv %>% select(-home_ownership))
czph <- cox.zph(model_base, terms = F)

par(mfrow=c(3,3),mar=c(4,4,2,2))
plot(czph, resid=FALSE)
```

Per molte variabili non vale l'assunzione di PH, le escludiamo. Alcune, sebbene graficamente sarebbero da escludere, sono invece tenute per via della scala. Ad esempio beta(t) di annual_inc è chiaramente non costante, tuttavia la variazione avviene su un ordine di grandezza di 1e-6 !!! 

```{r}
acc_ph <- acc_surv %>% select(-application_type, -initial_list_status, -int_rate, -num_rev_tl_bal_gt_0, -pct_tl_nvr_dlq, -purpose, -revol_bal, -revol_util, -term, -total_acc, -verification_status, -mo_sin_earliest_cr_line, -num_bc_not_sats, -home_ownership, -addr_state)
```

```{r}
model <- coxph(Surv(actual_duration_to_term, status) ~ ., acc_ph)
summary(model)
```

## Calibration

Si procede con la costruzione dei modelli, specificando l'opzione x = TRUE per poter poi utilizzare la funzione Score del pacchetto riskRegression.
Creo un modello aumentato con le variabili per cui non vale PH.

```{r}
library(riskRegression)
library(future)
library(foreach)
library(doFuture)
library(survival)
```

```{r}
# train.surv <- ...
model_base <- coxph(model$formula, data = acc_surv %>% select(-addr_state,-home_ownership), x = T,y=T)

model_aug <- coxph(Surv(actual_duration_to_term, status) ~ ., data = acc_surv %>% select(-addr_state,-home_ownership), x = T,y=T)

score <- Score(list("Base model"=model_base,"Augmented Model"=model_aug),
              formula=Surv(actual_duration_to_term, status)~1,
              data=acc_surv, conf.int=F,
              times=seq(0.1,0.9,0.2),
              plots=c("calibration","ROC"),
              progress.bar=1)

# internal cross-validation
CVscore <- Score(list("Base model"=model_base,"Augmented Model"=model_aug),
              formula=Surv(actual_duration_to_term, status)~1,
              data=acc_surv %>% select(-addr_state,-home_ownership), conf.int=F,
              times=c(0.5),
              #times=seq(0.1,0.9,0.2),
              plots="calibration",
              split.method="bootcv",B=10,M=100,seed=2021,
              parallel="multicore",
              progress.bar=1)
```

Si decide di prendere il time-point a metà del termine e si valutano le performance dei modelli sui tre aspetti: calibrazione, discriminazione, net benefit.

Per valutare la calibrazione dei modelli si usano insieme il Calibration Plot e il Brier Score.

```{r}
plotCalibration(score,times=0.5, cens.method="local",method="quantile", q=20, auc.in.legend = F)
title(main="Calibration at half term")

plotCalibration(CVscore,times=5,cens.method="local",method="quantile",q=10, auc.in.legend = F)
title(main="Calibration at half term")
```

### Discrimination

```{r}
plotROC(score,times=0.5,cens.method="local")
title(main="time-dependent ROC at half term")
```

```{r}
CVscore$AUC$score %>% filter(times == 0.5, model %in% c("Base Model", "Augmented Model")) %>% mutate(C.I = paste0("[", round(lower, 2), ",", round(upper,2), "]")) %>% select(model, AUC, C.I) %>% pander()
```

### Net Benefit

Quanto appena osservato ci indica che entrambi i modelli sono molto simili in quanto a calibrazione e capacità di discriminazione. \
Per arrivare infine a una valutazione dei modelli che ci consenta di affermare quale dei due sia migliore si stima il Net Benefit di ciascun modello e si confrontano i valori così ottenuti.


```{r include=FALSE}
source("data_preparation/stdca.R")

fit_base<-survfit(model_base,newdata=d)
d$riskdeath_base<- 1-as.numeric(summary(fit_base,times=5)$surv)
fit_aug<-survfit(model_aug,newdata=d)
d$riskdeath_aug<- 1-as.numeric(summary(fit_aug,times=5)$surv)

netbenefit <- stdca(data = d, outcome = "status", ttoutcome = "fuyrs", timepoint = 5, xstop = 0.5, 
                    predictors = c("riskdeath_base","riskdeath_aug"))
```

```{r}
par(mar = c(5, 4.5, 4, 1)) 

# Net Benefit for model_base:
plot(netbenefit$net.benefit$threshold, netbenefit$net.benefit$riskdeath_base, type = "l", lwd = 3, 
     xlim = c(0, 1), ylim = c(0, 0.2), xlab = "Riskdeath", ylab = "Net Benefit", xaxt = "n", yaxt = "n", 
     cex.lab = 1.7, cex.axis = 1.6, frame = F)
axis(1, at = seq(0,1,0.1), labels = NA, pos = 0)
axis(1, at = seq(0,1,0.1), labels = seq(0,1,0.1), cex.axis = 1.7, pos = 0)
axis(2, at = c(0,0.05,0.1,0.15,0.2), labels = c(0,0.05,0.1,0.15,0.2), cex.axis = 1.7, pos = 0)

# Net Benefit for model_aug:
lines(netbenefit$net.benefit$threshold, netbenefit$net.benefit$riskdeath_aug, type = "l", lty = 3, lwd = 3) 

# We add an extra axis on the top of the graph with the costs:
axis(3, at = c(0,0.2,0.4,0.6,0.8,1), labels = c(0,0.25,0.67,1.5,4,'Inf'), cex.axis = 1.6)
mtext("Cost", side = 3, line = 2.5, cex = 1.7, las = 1)

legend('topright', c("Base Model","Augmented Model"), lwd = c(3, 3), lty = c(1, 3), col = c(1, 1), bty = 'n', cex = 1.7)
```


# Tuning



# Conclusions


```{r}
score <- Score(list("LogReg"=fit.glm,
                    "LogReg PCA"=fit.pca,
                    "LogRidge"=fit.ridge.cv,
                    "LogRegLasso"=fit.lasso.cv,
                    "RandFor"=fit.rf.150t),
              formula=y~1,
              data=test, conf.int=F,
              plots=c("calibration","ROC"),
              progress.bar=1)

```


```{r fig.height=6, fig.width=10}
plot_precision_vs_npred(test$y, list("glm"=yhat.glm, "pca"=yhat.pca, "ridge"=yhat.ridge, "lasso"=yhat.lasso,"RandomForest"=yhat.rf[,2]), title = "Results")
```

```{r}
stopCluster(cl)
```



