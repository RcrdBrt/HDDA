---
title: "Progetto HDDA AA2021/2022"
output: html_notebook
---


# Libraries

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
if(!require(data.table)) install.packages("data.table")
if(!require(Hmisc)) install.packages("Hmisc")
if(!require(dplyr)) install.packages("dplyr")
if(!require(corrplot)) install.packages("corrplot")
if(!require(lubridate)) install.packages("lubridate")
if(!require(stringr)) install.packages("stringr")
if(!require(sampling)) install.packages("sampling")
if(!require(leaps)) install.packages("leaps")
if(!require(bestglm)) install.packages("bestglm")
if(!require(glmnet)) install.packages("glmnet")
if(!require(factoextra)) install.packages("factoextra")
if(!require(caret)) install.packages("caret")
if(!require(ranger)) install.packages("ranger")
if(!require(randomForest)) install.packages("randomForest")
if(!require(InformationValue)) install.packages("InformationValue")
if(!require(kernlab)) install.packages("kernlab")
if(!require(e1071)) install.packages("e1071")
if(!require(survival)) install.packages("survival")
if(!require(survminer)) install_github("kassambara/survminer", build_vignettes = FALSE)
if(!require(riskRegression)) install.packages("riskRegression")
library(data.table)
library(Hmisc)
library(dplyr)
library(corrplot)
library(lubridate)
library(stringr)
library(sampling)
library(leaps)
library(bestglm)
library(glmnet)
library(factoextra)
library(caret)
library(ranger)
library(randomForest) 
library(InformationValue) 
library(kernlab) 
library(e1071) 
library(survival)
library(riskRegression)
`%notin%` <- Negate(`%in%`)
```

# Data Ingestion

```{r message=FALSE, warning=FALSE, cache=TRUE, paged.print=FALSE}
acc_full <- fread("archive/accepted_2007_to_2018Q4.csv", sep = ",", header = T)
```

```{r, cache=TRUE}
variables <- c(
  "addr_state",
  "annual_inc",
  "application_type",
  "delinq_2yrs",
  "earliest_cr_line",
  "emp_length",
  #"emp_title", # troppi livelli, circa 600k
  "fico_range_high",
  "fico_range_low",
  "grade",
  "home_ownership",
  "initial_list_status",
  "int_rate",
  "issue_d",
  "last_pymnt_d",
  "loan_amnt",
  "funded_amnt",
  "loan_status",
  "mo_sin_old_il_acct",
  "mo_sin_old_rev_tl_op",
  "mort_acc",
  "num_bc_sats",
  "num_bc_tl",
  "num_op_rev_tl",
  "num_rev_accts",
  "num_rev_tl_bal_gt_0",
  "num_sats",
  "open_acc",
  "pct_tl_nvr_dlq",
  "percent_bc_gt_75",
  "pub_rec",
  "pub_rec_bankruptcies",
  "purpose",
  "revol_bal",
  "revol_util",
  "sub_grade",
  "tax_liens",
  "term",
  "total_acc",
  #"total_cu_tl",
  "verification_status"
)
```

```{r}
acc <- acc_full
acc <- acc[,..variables]
acc <- acc[complete.cases(acc),]
acc <- acc[!apply(acc, 1, function(x) any(x=="")),]
```

```{r}
fwrite(acc, "archive/accepted_clean.csv", sep = ",")
```

# Data Preparation
```{r}
acc <- fread("archive/accepted_clean.csv", sep=",",header=T)
acc <- as.data.frame(acc)
numeric_variables <- c(
  "annual_inc",
  "delinq_2yrs",
  "mo_sin_earliest_cr_line",
  "fico_range_high",
  "fico_range_low",
  "int_rate",
  "loan_amnt",
  "funded_amnt",
  "mo_sin_old_il_acct",
  "mo_sin_old_rev_tl_op",
  "mort_acc",
  "num_bc_sats",
  "num_bc_tl",
  "num_op_rev_tl",
  "num_rev_accts",
  "num_rev_tl_bal_gt_0",
  "num_sats",
  "open_acc",
  "pct_tl_nvr_dlq",
  "percent_bc_gt_75",
  "pub_rec",
  "pub_rec_bankruptcies",
  "revol_bal",
  "revol_util",
  "tax_liens",
  "total_acc"
)

acc <- acc %>%
  mutate(
    mo_sin_earliest_cr_line=as.integer(round((as.Date("2018-12-01")-lubridate::my(earliest_cr_line))/(365.25/12))),
    issue_d = lubridate::my(issue_d),
    last_pymnt_d = lubridate::my(last_pymnt_d),
    emp_length=as.numeric(str_extract(emp_length, "[0-9]+"))
  ) %>%
  mutate(
    emp_length = case_when(
      emp_length < 5 ~ "low",
      emp_length < 10 ~ "mid",
      T ~"high")) %>%
  select(-earliest_cr_line)

acc_with_current <- acc

acc <- acc %>% 
  filter(loan_status %notin% c("Current", "In Grace Period", "Late (16-30 days)")) %>%
  mutate(y=case_when(
    loan_status == "Late (31-120 days)" ~ 0,
    loan_status == "Charged Off" ~ 0,
    loan_status == "Default" ~ 0,
    loan_status == "Fully Paid" ~ 1
  )) %>%
  mutate(addr_state=case_when(
    addr_state %in% c("WA", "OR", "CA", "HI", "AL", "AK") ~ "West Coast",
    addr_state %in% c("MT", "ID", "NV", "UT", "CO", "WY") ~ "The Rocky Mountains",
    addr_state %in% c("AZ", "NM", "TX", "OK") ~ "South West",
    addr_state %in% c("ND", "SD", "NE", "KS", "MN", "IA", "MO", "WI", "IL", "IN", "MI", "OH") ~ "Midwest",
    addr_state %in% c("AR", "LA", "MS", "KY", "TN", "AL", "GA", "FL", "NC", "SC", "VA", "WV") ~ "South East",
    addr_state %in% c("ME", "VT", "NH", "MA", "RI", "CT") ~ "New England",
    addr_state %in% c("NY", "PA", "NJ", "MD", "DE", "DC") ~ "Mid Atlantic",
  )) %>%
  # livelli erano molto sbilanciati, optato per 3 livelli
  mutate(purpose=case_when(
    purpose %in% c("debt_consolidation") ~ "debt_consolidation",
    purpose %in% c("credit_card") ~ "credit_card",
    T ~ "other"
  )) %>% 
  mutate(
    addr_state=as.factor(addr_state),
    y=as.factor(y),
    application_type=as.factor(application_type),
    emp_length=as.factor(emp_length),
    grade=as.factor(grade),
    home_ownership=as.factor(home_ownership),
    initial_list_status=as.factor(initial_list_status),
    purpose=as.factor(purpose),
    sub_grade=as.factor(sub_grade),
    term=as.factor(term),
    verification_status=as.factor(verification_status),
  ) %>% select(-loan_status)
set.seed(1000)
source("data_preparation/outliers.R", print.eval = T)
```

Possiamo pensare di utilizzare anche i Current tipo con 90% già ripagato. Ma ci servono? No!

# Data Exploration

### numero di osservazioni / tempo

```{r, fig.width=10, fig.height=6}
source("data_preparation/plot_osservazioni_e_tempo.R", print.eval = T)
```

### Pie chart default vs fully paid

```{r}
source("data_preparation/pie_chart_default_VS_fully_paid.R", print.eval = T)
piechart(acc,group="y") +
  scale_fill_manual(labels=c("Default", "Fully Paid"), values=c("darkred","darkgreen"))
```

### ROI boxplot/violinplot (for each grade?)

```{r}
source("data_preparation/ROI_violin.R", print.eval = T)
```


### Violin plot for each numeric variable and barplot for categorical variables

```{r, fig.width=10, fig.height=6}
source("data_preparation/all_violin.R", print.eval = T)
```


### Investigate correlations and fix


```{r}
corrplot(cor(acc[,numeric_variables]))
```

```{r}
cor.mtest <- function(mat, ...) {
    mat <- as.matrix(mat)
    n <- ncol(mat)
    p.mat<- matrix(NA, n, n)
    diag(p.mat) <- 0
    for (i in 1:(n - 1)) {
        for (j in (i + 1):n) {
            tmp <- cor.test(mat[, i], mat[, j], ...)
            p.mat[i, j] <- p.mat[j, i] <- tmp$p.value
        }
    }
  colnames(p.mat) <- rownames(p.mat) <- colnames(mat)
  p.mat
}
# matrix of the p-value of the correlation
chart_pic_file = tempfile(pattern="file", tmpdir=tempdir(), fileext = ".png")
p.mat <- cor.mtest(acc[,numeric_variables])
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
png(filename = chart_pic_file, width = 900, height = 900, type = "cairo")
corrplot::corrplot(cor(acc[,numeric_variables]), method="color", col=col(200),  
         type="upper", 
         addCoef.col = "black", # Add coefficient of correlation
         tl.col="black", tl.srt=45, tl.cex = .7, number.cex=0.75,  #Text label color and rotation
         # Combine with significance
         p.mat = p.mat, sig.level = 0.2, insig = "blank", 
         # hide correlation coefficient on the principal diagonal
         diag=T 
         )
```

```{r pressure, out.width= '100%'}
knitr::include_graphics(chart_pic_file)
```


```{r}
ids <- sample(c(1:nrow(acc)), 8e4)
plot(acc[ids,"mo_sin_earliest_cr_line"], acc[ids,"mo_sin_old_rev_tl_op"])
```

```{r}
plot(acc[ids, "num_sats"], acc[ids, "open_acc"])
```

```{r}
### categorizzazione fico_mean (https://www.investopedia.com/terms/f/ficoscore.asp)
acc <- acc %>% 
  mutate(fico_mean = (fico_range_high+fico_range_low)/2) %>%
  select(-fico_range_high, -fico_range_low) %>% 
  mutate(fico_class=case_when(
    fico_mean <= 580 ~ "Poor",
    fico_mean > 580 & fico_mean <= 669 ~ "Fair",
    fico_mean > 670 & fico_mean <= 739 ~ "Good",
    fico_mean > 740 & fico_mean <= 799 ~ "Very Good",
    fico_mean > 800 ~ "Exceptional"
  )) %>% select(-fico_mean) %>% mutate(fico_class=as.factor(fico_class))

acc <- acc %>% 
  mutate(num_not_sats = (open_acc - num_sats)/open_acc,
         num_bc_not_sats = (num_bc_tl - num_bc_sats)/num_bc_tl) %>%
  select(-num_sats, num_bc_sats)

# tolte le correlazioni, restano queste variabili esplicative numeriche
new_numeric_variables <- c(
  "annual_inc",
  "delinq_2yrs",
  "mo_sin_earliest_cr_line",
  "int_rate",
  "loan_amnt",
  "mo_sin_old_il_acct",
  "mo_sin_old_rev_tl_op",
  "mort_acc",
  "num_bc_not_sats",
  "num_bc_tl",
  "num_op_rev_tl",
  "num_rev_accts",
  "num_rev_tl_bal_gt_0",
  "num_not_sats",
  "open_acc",
  "pct_tl_nvr_dlq",
  "percent_bc_gt_75",
  "pub_rec",
  "pub_rec_bankruptcies",
  "revol_bal",
  "revol_util",
  "tax_liens",
  "total_acc"
)
corrplot(cor(acc[, new_numeric_variables]))
```


# Modeling

```{r}
model_variables = names(acc)[names(acc) %notin% c(
  "grade",
  "funded_amnt",
  "issue_d",
  "last_pymnt_d",
  "num_sats",
  "num_bc_sats",
  "sub_grade",
  "pub_rec_bankruptcies",
  "num_rev_accts",
  "mo_sin_old_rev_tl_op"
)]

model_acc <- acc[,model_variables] %>% relocate(y, .after = last_col())

model_acc[, new_numeric_variables[new_numeric_variables %in% model_variables]] = scale(model_acc[, new_numeric_variables[new_numeric_variables %in% model_variables]])

corrplot(cor(model_acc[, new_numeric_variables[new_numeric_variables %in% model_variables]]))
```


```{r}
set.seed(1000)
acc.balanced <- model_acc %>% group_by(y) %>% sample_n(min(summary(acc$y))) %>% ungroup()

train.balanced = sample_frac(acc.balanced, 0.8)
test.balanced = anti_join(acc.balanced,train.balanced)

```


```{r}
set.seed(1000)
fit.glm <- glm(y~., train.balanced, family="binomial")

yhat.glm <- predict(fit.glm, test.balanced)-1

confusionMatrix(test.balanced$y, yhat.glm)
```

```{r}
plotROC(test.balanced$y, yhat.glm)
```


```{r}
#PCA
set.seed(1000)
pca.res <- prcomp(scale(as.matrix(acc[,new_numeric_variables])))
fviz_eig(pca.res)

print(pca.res$sdev)

acc_pca <- as.data.frame(scale(as.matrix(acc[,new_numeric_variables])) %*% pca.res$rotation)[,1:8] # prime otto variabili
#acc_pca <- acc_pca %>% cbind(acc %>% select(-new_numeric_variables))
acc_pca <- acc_pca %>% cbind(acc%>%select(y))
```


```{r}
set.seed(1000)
acc.balanced.pca <- acc_pca %>% group_by(y) %>% sample_n(min(summary(acc_pca$y))) %>% ungroup()

train.balanced.pca <- sample_frac(acc.balanced.pca, 0.8)
test.balanced.pca <- anti_join(acc.balanced.pca,train.balanced.pca)

fit.pca <- glm(y~., train.balanced.pca, family="binomial")

yhat.pca <- predict(fit.pca, test.balanced.pca)

ytest.pca = test.balanced.pca$y

confusionMatrix(ytest.pca, yhat.pca)
```

```{r}
plotROC(ytest.pca, yhat.pca)
```


## Best Subset Selection (BSS)

```{r}
set.seed(1000)
fit.bss = step(fit.glm, direction = "backward", trace=T)

yhat.bss = predict(fit.bss, test.balanced %>% select(-y))

plotROC(test.balanced$y, yhat.bss)
```

## Ridge

```{r}
set.seed(1000)

Xlm.train <- model.matrix(fit.glm)[,-1]
fit.ridge <- glmnet(Xlm, train.balanced$y, alpha = 0, family="binomial", nlambda = 100)    # alpha=0: ridge

summary(fit.ridge)
```

```{r}
plot(fit.ridge, xvar="lambda")
```
```{r}
set.seed(1000)
K <- 10
fit.ridge.cv <- cv.glmnet(Xlm.train, train.balanced$y, alpha=0, nfolds = K, grouped = FALSE, family="binomial", type.measure = "auc")

plot(fit.ridge.cv)
```

```{r}
Xlm.test = model.matrix(glm(y ~ ., test.balanced, family = "binomial"))[,-1]

yhat.ridge.cv.1se = predict(fit.ridge.cv$glmnet.fit, Xlm.test, s = "lambda.1se") manually

confusionMatrix(test.balanced$y, yhat.ridge.cv.1se)
```
```{r}
plotROC(test.balanced$y, yhat.ridge.cv.1se)
```


## Lasso
```{r}
set.seed(1000)
K <- 10
fit.lasso.cv <- cv.glmnet(Xlm, train.balanced$y, alpha=1, nfolds = K, grouped = FALSE, family="binomial", type.measure="auc" ,parallel=TRUE)
plot(fit.lasso.cv)
coef(fit.lasso.cv)
```

```{r}
set.seed(1000)
yhat.lasso.cv.1se <- predict(fit.lasso.cv, Xlm.test, s = "lambda.1se")
confusionMatrix(
  test.balanced$y,
  yhat.lasso,
  threshold = InformationValue::optimalCutoff(test.balanced$y, yhat.lasso, optimiseFor = "misclasserror"))
plotROC(test.balanced$y, yhat.lasso.cv.1se)
```



## SVM

```{r}
set.seed(1000)

fit.svm = svm(y ~ ., train.balanced %>% select(annual_inc,y))

plot(fit.svm)

```


# Random Forest

```{r}
set.seed(1000)

acc.balanced.tiny <- model_acc %>% group_by(y) %>% sample_n(10000) %>% ungroup()

train.balanced.tiny = sample_frac(acc.balanced.tiny, 0.8)
test.balanced.tiny = anti_join(acc.balanced.tiny, train.balanced.tiny)

fit.rf = randomForest(y ~ ., train.balanced.tiny, mtry = sqrt(length(names(train.balanced.tiny))), ntree=200, importance=T)

varImpPlot(fit.rf, type=1)

tuneRF(train.balanced.tiny %>% select(-y), train.balanced.tiny$y, ntreeTry = 100)

plot(fit.rf)

yhat.rf = predict(fit.rf, test.balanced.tiny %>% select(-y), type="prob", norm.votes=T)

plotROC(test.balanced.tiny$y, yhat.rf[,2])
```


# Survival Analysis

In order to perform survival analysis we define three possible outcomes for a lending: "Fully Paid", "Default" and "Pre-Paid", whose meanings are pretty clear.

To distinguish between Regalur Payment and Early Payment we define a tolerance of -1 month since our dates are all represented as month-year.

```{r}
acc_surv <- acc
acc_surv <- acc_surv %>% 
  mutate(term=as.numeric(substring(as.character(term),0,2))) %>%
  mutate(actual_duration_to_term = as.numeric(last_pymnt_d - issue_d) / (term*30))
hist(as.numeric(acc_surv[acc$y==1, "actual_duration_to_term"]), col = "blue")
hist(as.numeric(acc_surv[acc$y==0, "actual_duration_to_term"]), col="red",add =T)
acc_surv_pie <- acc_surv %>%
  mutate(y = as.factor(case_when(
      y == 0 ~ 0, ### Deafult
      (y == 1) & (actual_duration_to_term < 0.8) ~ 2, ### Early Payment
      T ~ 1 ### Regular Payment
    ))
  )
piechart(acc_surv_pie, "y") +
  scale_fill_manual(labels=c("Default", "Early Payment", "Fully Paid"), values=c("darkred", "darkorange", "darkgreen"))
```

## Survival function
```{r}
acc_surv <- acc_surv %>%
  mutate(actual_duration_to_term = ifelse(y==0, actual_duration_to_term, 1)) %>%
  mutate(actual_duration_to_term = ifelse(actual_duration_to_term > 1, 1, actual_duration_to_term)) %>%
  mutate(status = ifelse(y==0,1,0)) # inverto default e fully paid per farli digerire a survfit
# ignoro gli early payment e i pagamenti in ritardo, settando il tempo a 1 per loro, intendendo che questi prestiti sono sopravvissuti fino alla fine.

hist(as.numeric(acc_surv[acc_surv$status==0, "actual_duration_to_term"]), col="blue", breaks=seq(0,1,0.01))
hist(as.numeric(acc_surv[acc_surv$status==1, "actual_duration_to_term"]), col = "red", breaks=seq(0,1,0.1), add =T)

fit <- survfit(Surv(actual_duration_to_term, status) ~ 1, data = acc_surv)
ggsurvplot(fit, xlab = "Years", legend= "none", ggtheme = theme_minimal())
summary(fit, times = 0.5)
```

## Cox

```{r}
acc_surv <- acc_surv %>% select(all_of(model_variables), actual_duration_to_term, status) %>% select(-y)
acc_surv[, new_numeric_variables[new_numeric_variables %in% names(acc_surv)]] <- scale(model_acc[, new_numeric_variables[new_numeric_variables %in% names(acc_surv)]])
acc_surv <- acc_surv %>% mutate(term = as.factor(as.character(term)))
```

### Valutare assunzione «Proportional Hazards»

Per valutare l'assunzione di Proportional Hazards per le covariate si procede dapprima con un check visuale della log(-log(Survival)), che valuta l'assunzione di PH per ogni covariata indipendentemente dalle altre, e successivamente ci si serve dei residui Schoenfeld per valutare l'assunzione di PH tenendo conto anche dalle altre covariate (modello multivariato).

1. log[-log(SKM (t))] --- Graphical check

2. Schoenfeld residuals --- Graphical check

Infine si valuta la forma funzionale assunta dalle variabili continue *age* e *creat*, confrontandola con la forma lineare.


#### 1. log[-log(SKM (t))] --- Graphical check

Per l'assunzione di PH abbiamo: $S_X(t) = S_0(t) e^{(\beta' X)}$
che può essere riscritta come: $log(-log(S_X(t))) - log(-log(S_0(t))) = \beta' X$
ne segue che la differenza tra i due logaritmi è costante rispetto al tempo se è valida l'assunzione di PH. 

Questo tipo di verifica può essere fatto unicamente sulle variabili non continue, poiché il $log(-log(S_0(t)))$ deve essere confrontato con $log(-log(S_X(t)))$ per ciascun valore di X su tutto l'intervallo temporale.

```{r}
par(mfrow=c(2,2),mar=c(4,4,2,2))

km.addr_state <- survfit(Surv(actual_duration_to_term, status) ~ addr_state, data=acc_surv)
plot(km.addr_state, fun="cloglog", ylab="log(-log(Survival))", xlab="log(time_to_term)", main="Check PH assumption of address state")

km.appl_type <- survfit(Surv(actual_duration_to_term, status) ~ application_type, data=acc_surv)
plot(km.appl_type, fun="cloglog", ylab="log(-log(Survival))", xlab="log(time_to_term)", main="Check PH assumption of application type")

km.emp_length <- survfit(Surv(actual_duration_to_term, status) ~ emp_length, data=acc_surv)
plot(km.emp_length, fun="cloglog", ylab="log(-log(Survival))", xlab="log(time_to_term)", main="Check PH assumption of employment length")

km.home_ownership <- survfit(Surv(actual_duration_to_term, status) ~ home_ownership, data=acc_surv)
plot(km.addr_state, fun="cloglog", ylab="log(-log(Survival))", xlab="log(time_to_term)", main="Check PH assumption of home ownership")

km.init_list <- survfit(Surv(actual_duration_to_term, status) ~ initial_list_status, data=acc_surv)
plot(km.init_list, fun="cloglog", ylab="log(-log(Survival))", xlab="log(time_to_term)", main="Check PH assumption of initial list status")

km.purpose <- survfit(Surv(actual_duration_to_term, status) ~ purpose, data=acc_surv)
plot(km.purpose, fun="cloglog", ylab="log(-log(Survival))", xlab="log(time_to_term)", main="Check PH assumption of purpose")

km.term <- survfit(Surv(actual_duration_to_term, status) ~ term, data=acc_surv)
plot(km.term, fun="cloglog", ylab="log(-log(Survival))", xlab="log(time_to_term)", main="Check PH assumption of term")

km.ver_status <- survfit(Surv(actual_duration_to_term, status) ~ verification_status, data=acc_surv)
plot(km.ver_status, fun="cloglog", ylab="log(-log(Survival))", xlab="log(time_to_term)", main="Check PH assumption of verification status")

km.fico_class <- survfit(Surv(actual_duration_to_term, status) ~ fico_class, data=acc_surv)
plot(km.fico_class, fun="cloglog", ylab="log(-log(Survival))", xlab="log(time_to_term)", main="Check PH assumption of fico class")
```

L'assunzione di PH pare valere per tutte le variabili categoriche, mentre sembra che non valga per la "fico class".

#### 2. Schoenfeld residuals --- Graphical check

I residui Schoenfeld sono definiti come: $r_j = x_j - E[x_j|R_j]$ \
dove $x_j$ è la covariata dell'individuo che *fallisce* al tempo $t_{(j)}$ e $R_j$ è il risk set al tempo $t_{(j)}$.
Ci si aspetta che il valore atteso dei residui sia 0, ne segue che l'andamento dei residui rispetto al tempo deve essere costante. Si plottano i residui Schoenfeld standardizzati.


```{r}
model_base <- coxph(Surv(actual_duration_to_term, status) ~ ., data = acc_surv %>% select(-home_ownership))
czph <- cox.zph(model_base, terms = F)

par(mfrow=c(3,3),mar=c(4,4,2,2))
plot(czph, resid=FALSE)
```

Per molte variabili non vale l'assunzione di PH, le escludiamo. Alcune, sebbene graficamente sarebbero da escludere, sono invece tenute per via della scala. Ad esempio beta(t) di annual_inc è chiaramente non costante, tuttavia la variazione avviene su un ordine di grandezza di 1e-6 !!! 

```{r}
acc_ph <- acc_surv %>% select(-application_type, -initial_list_status, -int_rate, -num_rev_tl_bal_gt_0, -pct_tl_nvr_dlq, -purpose, -revol_bal, -revol_util, -term, -total_acc, -verification_status, -mo_sin_earliest_cr_line, -num_bc_not_sats)
```

```{r}
model <- coxph(Surv(actual_duration_to_term, status) ~ ., acc_ph)
summary(model)

model <- coxph(Surv(actual_duration_to_term, status) ~ ., acc_ph %>% select(-home_ownership))
summary(model)

```

## Calibration

Si procede con la costruzione dei modelli, specificando l'opzione x = TRUE per poter poi utilizzare la funzione Score del pacchetto riskRegression.
Creo un modello aumentato con le variabili per cui non vale PH.

```{r}
model_base <- coxph(model$formula, data = acc_surv, x = T)

model_aug <- coxph(Surv(actual_duration_to_term, status) ~ ., data = acc_surv, x = T)

score <- Score(list("Base Model"=model_base,"Augmented Model"=model_aug),
              formula=Surv(actual_duration_to_term, status)~1,
              data=acc_surv, conf.int=T,
              times=seq(0,1,0.1),
              plots=c("calibration","ROC"))

# internal cross-validation
CVscore<- Score(list("Base Model"=model_base,"Augmented Model"=model_aug),
                formula=Surv(actual_duration_to_term, status)~1,
                data=acc_surv,conf.int=T,
                times=seq(0,1,0.1),
                plots="calibration",
                split.method="loob",B=100,seed=2021)
```

Si decide di prendere il time-point a metà del termine e si valutano le performance dei modelli sui tre aspetti: calibrazione, discriminazione, net benefit.

Per valutare la calibrazione dei modelli si usano insieme il Calibration Plot e il Brier Score.

```{r}
plotCalibration(score,times=0.5,cens.method="local",method="quantile",q=10, auc.in.legend = F)
title(main="Calibration at half term")

plotCalibration(CVscore,times=5,cens.method="local",method="quantile",q=10, auc.in.legend = F)
title(main="Calibration at half term")
```

### Discrimination

```{r}
plotROC(score,times=0.5,cens.method="local")
title(main="time-dependent ROC at half term")
```


```{r}
CVscore$AUC$score %>% filter(times == 5, model %in% c("Base Model", "Augmented Model")) %>% mutate(C.I = paste0("[", round(lower, 2), ",", round(upper,2), "]")) %>% select(model, AUC, C.I) %>% pander()
```


### Net Benefit

Quanto appena osservato ci indica che entrambi i modelli sono molto simili in quanto a calibrazione e capacità di discriminazione. \
Per arrivare infine a una valutazione dei modelli che ci consenta di affermare quale dei due sia migliore si stima il Net Benefit di ciascun modello e si confrontano i valori così ottenuti.


```{r include=FALSE}
source("data_preparation/stdca.R")

fit_base<-survfit(model_base,newdata=d)
d$riskdeath_base<- 1-as.numeric(summary(fit_base,times=5)$surv)
fit_aug<-survfit(model_aug,newdata=d)
d$riskdeath_aug<- 1-as.numeric(summary(fit_aug,times=5)$surv)

netbenefit <- stdca(data = d, outcome = "status", ttoutcome = "fuyrs", timepoint = 5, xstop = 0.5, 
                    predictors = c("riskdeath_base","riskdeath_aug"))
```

```{r}
par(mar = c(5, 4.5, 4, 1)) 

# Net Benefit for model_base:
plot(netbenefit$net.benefit$threshold, netbenefit$net.benefit$riskdeath_base, type = "l", lwd = 3, 
     xlim = c(0, 1), ylim = c(0, 0.2), xlab = "Riskdeath", ylab = "Net Benefit", xaxt = "n", yaxt = "n", 
     cex.lab = 1.7, cex.axis = 1.6, frame = F)
axis(1, at = seq(0,1,0.1), labels = NA, pos = 0)
axis(1, at = seq(0,1,0.1), labels = seq(0,1,0.1), cex.axis = 1.7, pos = 0)
axis(2, at = c(0,0.05,0.1,0.15,0.2), labels = c(0,0.05,0.1,0.15,0.2), cex.axis = 1.7, pos = 0)

# Net Benefit for model_aug:
lines(netbenefit$net.benefit$threshold, netbenefit$net.benefit$riskdeath_aug, type = "l", lty = 3, lwd = 3) 

# We add an extra axis on the top of the graph with the costs:
axis(3, at = c(0,0.2,0.4,0.6,0.8,1), labels = c(0,0.25,0.67,1.5,4,'Inf'), cex.axis = 1.6)
mtext("Cost", side = 3, line = 2.5, cex = 1.7, las = 1)

legend('topright', c("Base Model","Augmented Model"), lwd = c(3, 3), lty = c(1, 3), col = c(1, 1), bty = 'n', cex = 1.7)
```


# Tuning



# Conclusions