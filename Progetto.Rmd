---
title: "Progetto HDDA AA2021/2022"
output: html_notebook
---

# Libraries

```{r message=FALSE, warning=FALSE, paged.print=FALSE, include=F}
if(!require(data.table)) install.packages("data.table")
if(!require(Hmisc)) install.packages("Hmisc")
if(!require(dplyr)) install.packages("dplyr")
if(!require(tidyr)) install.packages("tidyr")
if(!require(corrplot)) install.packages("corrplot")
if(!require(lubridate)) install.packages("lubridate")
if(!require(stringr)) install.packages("stringr")
if(!require(sampling)) install.packages("sampling")
if(!require(leaps)) install.packages("leaps")
if(!require(bestglm)) install.packages("bestglm")
if(!require(glmnet)) install.packages("glmnet")
if(!require(factoextra)) install.packages("factoextra")
if(!require(caret)) install.packages("caret")
if(!require(ranger)) install.packages("ranger")
if(!require(randomForest)) install.packages("randomForest")
if(!require(InformationValue)) install.packages("InformationValue")
if(!require(kernlab)) install.packages("kernlab")
if(!require(e1071)) install.packages("e1071")
if(!require(survival)) install.packages("survival")
if(!require(survminer)) install_github("kassambara/survminer", build_vignettes = FALSE)
if(!require(riskRegression)) install.packages("riskRegression")
if(!require(parallel)) install.packages("doParallel")
if(!require(doMC)) install.packages("doMC")
if(!require(pander)) install.packages("pander")
if(!require(patchwork)) install.packages("patchwork")

library(data.table)
library(Hmisc)
library(tidyr)
library(dplyr)
library(corrplot)
library(lubridate)
library(stringr)
library(sampling)
library(leaps)
library(bestglm)
library(glmnet)
library(factoextra)
library(caret)
library(ranger)
library(randomForest) 
library(InformationValue) 
library(kernlab) 
library(e1071) 
library(survival)
library(riskRegression)
library(parallel)
library(doMC)
library(pander)
library(patchwork)
library(car)

`%notin%` <- Negate(`%in%`)

registerDoMC(cores = detectCores())
#cl = makePSOCKcluster(detectCores())
#registerDoParallel(cl)
```

# Useful functions

```{r, include=F}
piechart <- function(d, group, legend_title = "Outcome"){
  piepercent <- d %>% 
    group_by((!!sym(group))) %>% 
    summarise(n_group = n()) %>% 
    mutate(n_perc= round( n_group/nrow(d)* 100,2))
  
  ggplot(piepercent, aes(x = "", y = n_perc, fill = !!sym(group))) +
    geom_bar(width = 1, stat = "identity", color = "white") +
    coord_polar("y", start = 0) +
    geom_text(aes(label = n_perc), position = position_stack(vjust = 0.5),color = "white")+
    guides(fill=guide_legend(title=legend_title))+
    theme_void()
}


MyConfusionMatrix <- function(actuals, predictions, threshold=0.5, title=""){
  cm <- caret::confusionMatrix(as.factor(ifelse(predictions>threshold,1,0)), actuals, threshold, dnn=c("Prediction", "Actual"), positive="1", mode="prec_recall")
  ggplot(as.data.frame(cm$table), aes(Actual, Prediction, fill= Freq)) +
    geom_tile() + geom_text(aes(label=Freq), fontface="bold") +
    scale_fill_distiller(palette="Greens",guide="none", direction = 1) +
    labs(x = "Actual",y = "Prediction", title=title, subtitle=
           paste(
             "Precision at 0.5 threshold:",
             round(precision(actuals,predictions,threshold),3)*100, "%")) +
    scale_x_discrete(labels=c("Default", "Fully Paid")) +
    scale_y_discrete(labels=c("Fully Paid","Default"), limits=rev) +
    theme_minimal() + 
    theme(plot.title = element_text(hjust = 0.5, size = 16),
          plot.subtitle = element_text(hjust = 0.5, size = 12)) 
}


plot_precision_vs_npred <- function(actuals, predictions, title=""){
  p <- vector("list", length(predictions))
  names(p) <- names(predictions)
  n <- vector("list", length(predictions))
  names(n) <- names(predictions)
  for (i in 1:length(predictions)){
      for (threshold in seq(0,1,0.05)){
        p[[i]] <- c(p[[i]],precision(actuals, predictions[[i]], threshold = threshold))
        n[[i]] <- c(n[[i]],length(predictions[[i]][predictions[[i]] >= threshold]))
      }
  }
  
  res <- as.data.frame(list(p=p,n=n))
  res$i <- 1:nrow(res)
  res <- gather(res,var,val,-i) %>% separate(var, c("variable", "Model")) %>% reshape(idvar = c("i","Model"), timevar = "variable", direction = "wide")
  names(res)[c(3,4)] <- c("p","n")
  
  plotly::plot_ly(res, x=~p*100, y = ~n/length(actuals)*100, type="scatter", mode = "lines+markers", color=~Model, opacity = 1) %>% 
  plotly::layout(title = "Results",
                 xaxis = list(title = 'Precision',  ticksuffix = "%"),
                 yaxis = list(title = 'Number of predicted Fully Paid', ticksuffix = "%"),
                 legend = list(x=0.8,y=0.9))

}
```

# Data Ingestion

```{r message=FALSE, warning=FALSE, cache=TRUE, paged.print=FALSE}
acc_full <- fread("archive/accepted_2007_to_2018Q4.csv", sep = ",", header = T)
```

```{r, cache=TRUE}
variables <- c(
  "addr_state",
  "annual_inc",
  "application_type",
  "delinq_2yrs",
  "earliest_cr_line",
  "emp_length",
  #"emp_title", # troppi livelli, circa 600k
  "fico_range_high",
  "fico_range_low",
  "grade",
  "home_ownership",
  "initial_list_status",
  "int_rate",
  "issue_d",
  "last_pymnt_d",
  "loan_amnt",
  "funded_amnt",
  "loan_status",
  "mo_sin_old_il_acct",
  "mo_sin_old_rev_tl_op",
  "mort_acc",
  "num_bc_sats",
  "num_bc_tl",
  "num_op_rev_tl",
  "num_rev_accts",
  "num_rev_tl_bal_gt_0",
  "num_sats",
  "open_acc",
  "pct_tl_nvr_dlq",
  "percent_bc_gt_75",
  "pub_rec",
  "pub_rec_bankruptcies",
  "purpose",
  "revol_bal",
  "revol_util",
  "sub_grade",
  "tax_liens",
  "term",
  "total_acc",
  #"total_cu_tl",
  "verification_status"
)
```

```{r}
acc <- acc_full
acc <- acc[,..variables]
acc <- acc[complete.cases(acc),]
acc <- acc[!apply(acc, 1, function(x) any(x=="")),]
```

```{r}
fwrite(acc, "archive/accepted_clean.csv", sep = ",")
```

# Data Preparation
```{r}
acc <- fread("archive/accepted_clean.csv", sep=",",header=T)
acc <- as.data.frame(acc)
numeric_variables <- c(
  "annual_inc",
  "delinq_2yrs",
  "mo_sin_earliest_cr_line",
  "fico_range_high",
  "fico_range_low",
  "int_rate",
  "loan_amnt",
  "funded_amnt",
  "mo_sin_old_il_acct",
  "mo_sin_old_rev_tl_op",
  "mort_acc",
  "num_bc_sats",
  "num_bc_tl",
  "num_op_rev_tl",
  "num_rev_accts",
  "num_rev_tl_bal_gt_0",
  "num_sats",
  "open_acc",
  "pct_tl_nvr_dlq",
  "percent_bc_gt_75",
  "pub_rec",
  "pub_rec_bankruptcies",
  "revol_bal",
  "revol_util",
  "tax_liens",
  "total_acc"
)

acc <- acc %>%
  mutate(
    mo_sin_earliest_cr_line=as.integer(round((as.Date("2018-12-01")-lubridate::my(earliest_cr_line))/(365.25/12))),
    issue_d = lubridate::my(issue_d),
    last_pymnt_d = lubridate::my(last_pymnt_d),
    emp_length=as.numeric(str_extract(emp_length, "[0-9]+"))
  ) %>%
  mutate(
    emp_length = case_when(
      emp_length < 5 ~ "low",
      emp_length < 10 ~ "mid",
      T ~"high")) %>%
  select(-earliest_cr_line)

acc_with_current <- acc

acc <- acc %>% 
  filter(loan_status %notin% c("Current", "In Grace Period", "Late (16-30 days)")) %>%
  mutate(y=case_when(
    loan_status == "Late (31-120 days)" ~ 0,
    loan_status == "Charged Off" ~ 0,
    loan_status == "Default" ~ 0,
    loan_status == "Fully Paid" ~ 1
  )) %>%
  mutate(addr_state=case_when(
    addr_state %in% c("WA", "OR", "CA", "HI", "AL", "AK") ~ "West Coast",
    addr_state %in% c("MT", "ID", "NV", "UT", "CO", "WY") ~ "The Rocky Mountains",
    addr_state %in% c("AZ", "NM", "TX", "OK") ~ "South West",
    addr_state %in% c("ND", "SD", "NE", "KS", "MN", "IA", "MO", "WI", "IL", "IN", "MI", "OH") ~ "Midwest",
    addr_state %in% c("AR", "LA", "MS", "KY", "TN", "AL", "GA", "FL", "NC", "SC", "VA", "WV") ~ "South East",
    addr_state %in% c("ME", "VT", "NH", "MA", "RI", "CT") ~ "New England",
    addr_state %in% c("NY", "PA", "NJ", "MD", "DE", "DC") ~ "Mid Atlantic",
  )) %>%
  # livelli erano molto sbilanciati, optato per 3 livelli
  mutate(purpose=case_when(
    purpose %in% c("debt_consolidation") ~ "debt_consolidation",
    purpose %in% c("credit_card") ~ "credit_card",
    T ~ "other"
  )) %>% 
  mutate(
    addr_state=as.factor(addr_state),
    y=as.factor(y),
    application_type=as.factor(application_type),
    emp_length=as.factor(emp_length),
    grade=as.factor(grade),
    home_ownership=as.factor(home_ownership),
    initial_list_status=as.factor(initial_list_status),
    purpose=as.factor(purpose),
    sub_grade=as.factor(sub_grade),
    term=as.factor(term),
    verification_status=as.factor(verification_status)
  ) %>% 
  filter(home_ownership %notin% c("ANY", "NONE", "OTHER"))  %>%
  mutate(home_ownership = as.factor(as.character(home_ownership))) %>% # remove empty levels
  select(-loan_status)
set.seed(1000)
source("data_preparation/outliers.R", print.eval = T)
```

Possiamo pensare di utilizzare anche i Current tipo con 90% gi√† ripagato. Ma ci servono? No!

# Data Exploration

### numero di osservazioni / tempo

```{r, fig.width=10, fig.height=6}
source("data_preparation/plot_osservazioni_e_tempo.R", print.eval = T)
```

### Pie chart default vs fully paid

```{r}
piechart(acc,group="y") +
  scale_fill_manual(labels=c("Default", "Fully Paid"), values=c("darkred","darkgreen"))
```

### ROI boxplot/violinplot (for each grade?)

```{r}
source("data_preparation/ROI_violin.R", print.eval = T)
```


### Violin plot for each numeric variable and barplot for categorical variables

```{r, fig.width=10, fig.height=6}
source("data_preparation/all_violin.R", print.eval = T)
```


### Investigate correlations and fix


```{r}
corrplot(cor(acc[,numeric_variables]))
```

```{r}
cor.mtest <- function(mat, ...) {
    mat <- as.matrix(mat)
    n <- ncol(mat)
    p.mat<- matrix(NA, n, n)
    diag(p.mat) <- 0
    for (i in 1:(n - 1)) {
        for (j in (i + 1):n) {
            tmp <- cor.test(mat[, i], mat[, j], ...)
            p.mat[i, j] <- p.mat[j, i] <- tmp$p.value
        }
    }
  colnames(p.mat) <- rownames(p.mat) <- colnames(mat)
  p.mat
}
# matrix of the p-value of the correlation
chart_pic_file = tempfile(pattern="file", tmpdir=tempdir(), fileext = ".png")
p.mat <- cor.mtest(acc[,numeric_variables])
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
png(filename = chart_pic_file, width = 900, height = 900, type = "cairo")
corrplot::corrplot(cor(acc[,numeric_variables]), method="color", col=col(200),  
         type="upper", 
         addCoef.col = "black", # Add coefficient of correlation
         tl.col="black", tl.srt=45, tl.cex = .7, number.cex=0.75,  #Text label color and rotation
         # Combine with significance
         p.mat = p.mat, sig.level = 0.2, insig = "blank", 
         # hide correlation coefficient on the principal diagonal
         diag=T 
         )
```

```{r pressure, out.width= '100%'}
knitr::include_graphics(chart_pic_file)
```

```{r}
ids <- sample(c(1:nrow(acc)), 8e4)
plot(acc[ids,"mo_sin_earliest_cr_line"], acc[ids,"mo_sin_old_rev_tl_op"])
```

```{r}
plot(acc[ids, "num_sats"], acc[ids, "open_acc"])
```

# Fix Correlation

```{r}
### categorizzazione fico_mean (https://www.investopedia.com/terms/f/ficoscore.asp)
acc <- acc %>% 
  mutate(fico_mean = (fico_range_high+fico_range_low)/2) %>%
  select(-fico_range_high, -fico_range_low) %>% 
  mutate(fico_class=case_when(
    fico_mean <= 580 ~ "Poor",
    fico_mean > 580 & fico_mean <= 669 ~ "Fair",
    fico_mean > 670 & fico_mean <= 739 ~ "Good",
    fico_mean > 740 & fico_mean <= 799 ~ "Very Good",
    fico_mean > 800 ~ "Exceptional"
  )) %>% select(-fico_mean) %>% mutate(fico_class=as.factor(fico_class))

acc <- acc %>% 
  mutate(num_not_sats = (open_acc - num_sats)/open_acc,
         num_bc_not_sats = (num_bc_tl - num_bc_sats)/num_bc_tl) %>%
  select(-num_sats, num_bc_sats)

# tolte le correlazioni, restano queste variabili esplicative numeriche
new_numeric_variables <- c(
  "annual_inc",
  "delinq_2yrs",
  "mo_sin_earliest_cr_line",
  "int_rate",
  "loan_amnt",
  "mo_sin_old_il_acct",
  "mo_sin_old_rev_tl_op",
  "mort_acc",
  "num_bc_not_sats",
  "num_bc_tl",
  "num_op_rev_tl",
  "num_rev_accts",
  "num_rev_tl_bal_gt_0",
  "num_not_sats",
  "open_acc",
  "pct_tl_nvr_dlq",
  "percent_bc_gt_75",
  "pub_rec",
  "pub_rec_bankruptcies",
  "revol_bal",
  "revol_util",
  "tax_liens",
  "total_acc"
)
corrplot(cor(acc[, new_numeric_variables]))
```


# Modeling

```{r}
model_variables <-  names(acc)[names(acc) %notin% c(
  "grade",
  "funded_amnt",
  "issue_d",
  "last_pymnt_d",
  "num_sats",
  "num_bc_sats",
  "sub_grade",
  "pub_rec_bankruptcies",
  "num_rev_accts",
  "mo_sin_old_rev_tl_op"
)]

model_num_variables <- model_variables[model_variables %in% new_numeric_variables]

model_acc <- acc[,model_variables] %>% relocate(y, .after = last_col())

model_acc[, model_num_variables] <- scale(model_acc[, model_num_variables])

corrplot(cor(model_acc[, model_num_variables]))
```

```{r message=F}
set.seed(1000)
train <- sample_frac(model_acc, 0.7)
test <- anti_join(model_acc,train)
```

To deal with class imbalance we define weights so that the model's predictions will be fair to both classes.

```{r}
model_weights <- ifelse(train$y == "0",
                        (1/table(train$y)[1]) * 0.5, ### (1 / numero osservazioni classe 0) / 2
                        (1/table(train$y)[2]) * 0.5) ### (1 / numero osservazioni classe 1) / 2
# sum(model_weights) == 1 TRUE
```

## Logistic Regression

### Plain

```{r}
fit.glm.unbalanced <- glm(y~., train, family="binomial")
vif(fit.glm.unbalanced)
```


```{r}
yhat.glm.unbalanced <- predict(fit.glm.unbalanced, test, type="response")
cat("Train Accuracy of Unbalanced Model:", 1-InformationValue::misClassError(train$y, fit.glm.unbalanced$fitted.values), "\n")
cat("Test Accuracy of Unbalanced Model:", 1-InformationValue::misClassError(test$y, yhat.glm.unbalanced))
```

```{r}
train.under <- train %>% group_by(y) %>% slice_sample(n=min(table(train$y))) %>% ungroup()
fit.glm.under <- glm(y~., train.under, family="binomial")
yhat.glm.under<- predict(fit.glm.under, test, type="response")
cat("Train Accuracy of UnderSampling Model:", 1-InformationValue::misClassError(train.under$y, fit.glm.under$fitted.values), "\n")
cat("Test Accuracy of UnderSampling Model:", 1-InformationValue::misClassError(test$y, yhat.glm.under))
```

```{r}
fit.glm <- glm(y~., train, family="binomial", weights=model_weights)
yhat.glm <- predict(fit.glm, test, type="response")
cat("Train Accuracy of Balanced Model:", 1-InformationValue::misClassError(train$y, fit.glm$fitted.values), "\n")
cat("Test Accuracy of Balanced Model:", 1-InformationValue::misClassError(test$y, yhat.glm))
```


```{r fig.width=10, fig.height=4}
p1 <- MyConfusionMatrix(train$y, test$y, fit.glm.unbalanced$fitted.values, yhat.glm.unbalanced, title="Logit Unbalanced")
p2 <- MyConfusionMatrix(train.under$y, test$y, fit.glm.under$fitted.values, yhat.glm.under, title="Logit UnderSampling")
p3 <- MyConfusionMatrix(train$y, test$y, fit.glm$fitted.values, yhat.glm, title="Logit Weighted")
p1+p2+p3
```

L'utilizzo dei pesi ci permette di avere previsioni pi√π bilanciate, al costo di peggiorare l'accuracy totale del nostro modello si riescono ad ottenere previsioni pi√π attendibili sulla classe minoritaria (Default). Si osserva infatti come il modello senza il bilanciamento ha un'accuracy sulla classe Default del 9%, mentre il modello bilanciato arriva al 63%. In conclusione otteniamo una Precision sulla classe Fully Paid pi√π elevata.
Da qui in avanti verranno prodotti solo modelli bilanciati tramite l'utilizzo dei pesi.

```{r}
InformationValue::plotROC(test$y, yhat.glm.under)
```

Per avere il modello il pi√π parsimonioso possibile, viene selezionato il training set con undersampling in quanto quello pesato, con un numero di osservazioni di gran lunga superiore, non apporta un apprezzabile incremento di precisione.

```{r}
train <- train.under
```

### PCA

```{r}
pca.res <- prcomp(train[, model_num_variables])

plot(x=1:length(pca.res$sdev), y=pca.res$sdev, type="p", col=ifelse(pca.res$sdev>=1,"red","blue") , xlab="# components", ylab="Standard Deviation") # si prendono quelle > 1
abline(h=1, col="red")
fviz_eig(pca.res, chioce="eigenvalue")
```


```{r}
train.pca <- as.data.frame(as.matrix(train[,model_num_variables]) %*% pca.res$rotation)[,1:7] # prime sette variabili
test.pca <- as.data.frame(as.matrix(test[,model_num_variables]) %*% pca.res$rotation)[,1:7] # prime sette variabili

train.pca$y <- train$y
test.pca$y <- test$y
```

```{r}
fit.pca <- glm(y~., train.pca, family="binomial")

yhat.pca <- predict(fit.pca, test.pca, type="response")

MyConfusionMatrix(train.pca$y, test.pca$y, fit.pca$fitted.values, yhat.pca, title = "Logit - PCA")
cat("Train Accuracy = ", 1-InformationValue::misClassError(train.pca$y, fit.pca$fitted.values), "\n")
cat("Test Accuracy = ", 1-InformationValue::misClassError(test.pca$y, yhat.pca))
```

```{r}
InformationValue::plotROC(test.pca$y, yhat.pca)
```

Cerchiamo quindi altri metodi blah blah.

### Backward Elimination

```{r}
train.tiny <- train %>% slice_sample(n=5e4) %>% ungroup()

fit.glm.tiny <- glm(y~., train.tiny, family="binomial")

glm.backward = step(fit.glm.tiny, direction = "backward", trace=T, k=2)

fit.glm.backward <- glm(glm.backward$formula, train, family = "binomial")

yhat.glm.backward = predict(fit.glm.backward, test, type="response")

MyConfusionMatrix(test$y, yhat.glm.backward, title = "Logit - Backward Elimination")

cat("Train Accuracy = ", 1-InformationValue::misClassError(train$y, fit.glm.backward$fitted.values), "\n")
cat("Test Accuracy = ", 1-InformationValue::misClassError(test$y, yhat.glm.backward))
```

```{r}
InformationValue::plotROC(test$y, yhat.glm.backward)
```

Backward e' troppo pesante, cerchiamo altri metodi

### Ridge

```{r}
X.train <- model.matrix(y~., train)[,-1]
K <- 10
fit.ridge.cv <- cv.glmnet(X.train, train$y, alpha=0, family="binomial", nfolds = K, grouped = FALSE, type.measure = "class", parallel = T)
plot(fit.ridge.cv)
```

Perch√® abbiamo usato cv.glmnet??? Perch√® vogliamo mostrare come la crossvalidation sia nel nostro caso piuttosto inutile, avendo un dataset molto grande i risultati sulle varie folds sono estremamente stabili/simili. Cos√¨ abbiamo giustificato l'utilizzo del metodo holdout invece della cross-validation.

```{r}
X.test <- model.matrix(y~., test)[,-1]

yhat.ridge = predict(fit.ridge.cv, X.test, s = "lambda.1se",  type="response")

MyConfusionMatrix(test$y, yhat.ridge, threshold = 0.5, title = "Ridge Logit")
cat("Accuracy = ", 1-InformationValue::misClassError(test$y, yhat.ridge, threshold = 0.5))
```


```{r}
InformationValue::plotROC(test$y, yhat.ridge)
```

Commento.
Grafico confronto coefficienti.

### Lasso

```{r}
K <- 10
fit.lasso.cv <- cv.glmnet(X.train, train$y, alpha=1, family="binomial", nfolds = K, grouped = FALSE, type.measure = "class", parallel = T)
plot(fit.lasso.cv)
```

```{r}
cat(paste(c("Variables excluded by lasso selection are:", rownames(coef(fit.lasso.cv, s = "lambda.1se"))[(coef(fit.lasso.cv, s = "lambda.1se") == 0)[,1]]), collapse=" \n" ))
```

```{r}
yhat.lasso = predict(fit.lasso.cv, X.test, s = "lambda.1se",  type="response")

MyConfusionMatrix(test$y, yhat.lasso, threshold = 0.5, title="Lasso Logit")
cat("Accuracy = ", 1-InformationValue::misClassError(test$y, yhat.lasso, threshold = 0.5))
```

```{r}
InformationValue::plotROC(test$y, yhat.lasso)
```

La lasso e' bella!

# Random Forest

```{r}
customTuneRF<- function (data, ntree, model_weights, mtryStart) 
{
  data <- data %>% group_by(y) %>% slice_sample(n=1e5) %>% ungroup() 
  x <- data %>% select(-y)
  y <- data$y
  tuning_data <- data %>% group_by(y) %>% slice_sample(n=5e4) %>% ungroup()
  tuning_x <- tuning_data %>% select(-y)
  tuning_y <- tuning_data$y
  class_weights <- unique(model_weights)
  class_weights <- c("0" = max(class_weights), "1" = min(class_weights))
  tune <- tuneRF(tuning_x, tuning_y, trace=T, strata=tuning_y, classwt=class_weights, mtryStart=mtryStart)
  print("Tuning is over!")
  randomForest(x, y, mtry=tune[which.min(tune[, 
      2]), 1], ntree=ntree, strata=y, classwt=class_weights, importance=T, do.trace=T)
}
```

```{r}
fit.rf.150t = customTuneRF(train, ntree=150, model_weights, mtryStart=7)
```

```{r}
plot(fit.rf.150t, lty=c(1,2,3))
legend("topright", inset=.05,cex = 0.7, 
  c("OOB Mean","OOB Default","OOB Fully paid"), col=c("black", "red", "green"), lwd=2, lty=c(1,2,3), horiz=TRUE)
```

```{r}
varImpPlot(fit.rf.150t, type=1, scale=T)
#abline(v=0, col="red")
```

Controllare presenza variabili che peggiorano e in caso girare nuovo randomforest senza di esse.

```{r}
yhat.rf = predict(fit.rf.150t, test %>% select(-y), type="prob", norm.votes=T)

MyConfusionMatrix(test$y, yhat.rf[,2], threshold = 0.5)

cat("Accuracy = ", 1-InformationValue::misClassError(test$y, yhat.rf[,2], threshold = 0.5))
```

```{r}
InformationValue::plotROC(test$y, yhat.rf[,2])
```


# Survival Analysis

In order to perform survival analysis we define three possible outcomes for a lending: "Fully Paid", "Default" and "Pre-Paid", whose meanings are pretty clear.

To distinguish between Regalur Payment and Early Payment we define a tolerance of -1 month since our dates are all represented as month-year.

```{r}
acc_surv <- acc
acc_surv <- acc_surv %>% 
  mutate(actual_duration_to_term = as.numeric(last_pymnt_d - issue_d) / (as.numeric(substring(as.character(term),0,2))*30))
hist(as.numeric(acc_surv[acc$y==0, "actual_duration_to_term"]), col = "darkgreen")
hist(as.numeric(acc_surv[acc$y==1, "actual_duration_to_term"]), col="darkred",add =T)
acc_surv_pie <- acc_surv %>%
  mutate(y = as.factor(case_when(
      y == 1 ~ 1, ### Default
      (y == 0) & (actual_duration_to_term < 0.8) ~ -1, ### Early Payment
      T ~ 0 ### Regular Payment
    ))
  )
piechart(acc_surv_pie, "y") +
  scale_fill_manual(labels=c("Regular Payment", "Early Payment", "Default"), values=c("darkgreen", "darkgray", "darkred"))
```

## Survival function

```{r}
acc_surv <- acc_surv %>%
  mutate(actual_duration_to_term = ifelse(y==1, actual_duration_to_term, 1)) %>%
  mutate(actual_duration_to_term = ifelse(actual_duration_to_term > 1, 1, actual_duration_to_term)) %>%
  mutate(status = as.numeric(as.character(y)))
# ignoro gli early payment e i pagamenti in ritardo, settando il tempo a 1 per loro, intendendo che questi prestiti sono sopravvissuti fino alla fine.

hist(as.numeric(acc_surv[acc_surv$y==0, "actual_duration_to_term"]), col="darkgreen", breaks=seq(0,1,0.01))
hist(as.numeric(acc_surv[acc_surv$y==1, "actual_duration_to_term"]), col = "darkred", breaks=seq(0,1,0.05),add=T)

fit <- survfit(Surv(actual_duration_to_term, status) ~ 1, data = acc_surv)
ggsurvplot(fit, xlab = "Years", legend= "none", ggtheme = theme_minimal())
summary(fit, times = 0.5)
```

## Cox

```{r}
acc_surv <- acc_surv %>% select(all_of(model_variables), actual_duration_to_term, status) %>% select(-y)
acc_surv[, new_numeric_variables[new_numeric_variables %in% names(acc_surv)]] <- scale(model_acc[, new_numeric_variables[new_numeric_variables %in% names(acc_surv)]])
```

### Valutare assunzione ¬´Proportional Hazards¬ª

Per valutare l'assunzione di Proportional Hazards per le covariate si procede dapprima con un check visuale della log(-log(Survival)), che valuta l'assunzione di PH per ogni covariata indipendentemente dalle altre, e successivamente ci si serve dei residui Schoenfeld per valutare l'assunzione di PH tenendo conto anche dalle altre covariate (modello multivariato).

1. log[-log(SKM (t))] --- Graphical check

2. Schoenfeld residuals --- Graphical check

Infine si valuta la forma funzionale assunta dalle variabili continue *age* e *creat*, confrontandola con la forma lineare.


#### 1. log[-log(SKM (t))] --- Graphical check

Per l'assunzione di PH abbiamo: $S_X(t) = S_0(t) e^{(\beta' X)}$
che pu√≤ essere riscritta come: $log(-log(S_X(t))) - log(-log(S_0(t))) = \beta' X$
ne segue che la differenza tra i due logaritmi √® costante rispetto al tempo se √® valida l'assunzione di PH. 

Questo tipo di verifica pu√≤ essere fatto unicamente sulle variabili non continue, poich√© il $log(-log(S_0(t)))$ deve essere confrontato con $log(-log(S_X(t)))$ per ciascun valore di X su tutto l'intervallo temporale.

```{r}
par(mfrow=c(2,2),mar=c(4,4,2,2))

km.addr_state <- survfit(Surv(actual_duration_to_term, status) ~ addr_state, data=acc_surv)
plot(km.addr_state, fun="cloglog", ylab="log(-log(Survival))", xlab="log(time_to_term)", main="Check PH assumption of address state")

km.appl_type <- survfit(Surv(actual_duration_to_term, status) ~ application_type, data=acc_surv)
plot(km.appl_type, fun="cloglog", ylab="log(-log(Survival))", xlab="log(time_to_term)", main="Check PH assumption of application type")

km.emp_length <- survfit(Surv(actual_duration_to_term, status) ~ emp_length, data=acc_surv)
plot(km.emp_length, fun="cloglog", ylab="log(-log(Survival))", xlab="log(time_to_term)", main="Check PH assumption of employment length")

km.home_ownership <- survfit(Surv(actual_duration_to_term, status) ~ home_ownership, data=acc_surv)
plot(km.addr_state, fun="cloglog", ylab="log(-log(Survival))", xlab="log(time_to_term)", main="Check PH assumption of home ownership")

km.init_list <- survfit(Surv(actual_duration_to_term, status) ~ initial_list_status, data=acc_surv)
plot(km.init_list, fun="cloglog", ylab="log(-log(Survival))", xlab="log(time_to_term)", main="Check PH assumption of initial list status")

km.purpose <- survfit(Surv(actual_duration_to_term, status) ~ purpose, data=acc_surv)
plot(km.purpose, fun="cloglog", ylab="log(-log(Survival))", xlab="log(time_to_term)", main="Check PH assumption of purpose")

km.term <- survfit(Surv(actual_duration_to_term, status) ~ term, data=acc_surv)
plot(km.term, fun="cloglog", ylab="log(-log(Survival))", xlab="log(time_to_term)", main="Check PH assumption of term")

km.ver_status <- survfit(Surv(actual_duration_to_term, status) ~ verification_status, data=acc_surv)
plot(km.ver_status, fun="cloglog", ylab="log(-log(Survival))", xlab="log(time_to_term)", main="Check PH assumption of verification status")

km.fico_class <- survfit(Surv(actual_duration_to_term, status) ~ fico_class, data=acc_surv)
plot(km.fico_class, fun="cloglog", ylab="log(-log(Survival))", xlab="log(time_to_term)", main="Check PH assumption of fico class")
```

L'assunzione di PH pare valere per tutte le variabili categoriche, mentre sembra che non valga per la "fico class".

#### 2. Schoenfeld residuals --- Graphical check

I residui Schoenfeld sono definiti come: $r_j = x_j - E[x_j|R_j]$ \
dove $x_j$ √® la covariata dell'individuo che *fallisce* al tempo $t_{(j)}$ e $R_j$ √® il risk set al tempo $t_{(j)}$.
Ci si aspetta che il valore atteso dei residui sia 0, ne segue che l'andamento dei residui rispetto al tempo deve essere costante. Si plottano i residui Schoenfeld standardizzati.


```{r}
model_base <- coxph(Surv(actual_duration_to_term, status) ~ ., data = acc_surv %>% select(-home_ownership))
czph <- cox.zph(model_base, terms = F)

par(mfrow=c(3,3),mar=c(4,4,2,2))
plot(czph, resid=FALSE)
```

Per molte variabili non vale l'assunzione di PH, le escludiamo. Alcune, sebbene graficamente sarebbero da escludere, sono invece tenute per via della scala. Ad esempio beta(t) di annual_inc √® chiaramente non costante, tuttavia la variazione avviene su un ordine di grandezza di 1e-6 !!! 

```{r}
acc_ph <- acc_surv %>% select(-application_type, -initial_list_status, -int_rate, -num_rev_tl_bal_gt_0, -pct_tl_nvr_dlq, -purpose, -revol_bal, -revol_util, -term, -total_acc, -verification_status, -mo_sin_earliest_cr_line, -num_bc_not_sats, -home_ownership, -addr_state)
```

```{r}
model <- coxph(Surv(actual_duration_to_term, status) ~ ., acc_ph)
summary(model)
```

## Calibration

Si procede con la costruzione dei modelli, specificando l'opzione x = TRUE per poter poi utilizzare la funzione Score del pacchetto riskRegression.
Creo un modello aumentato con le variabili per cui non vale PH.

```{r}
library(riskRegression)
library(future)
library(foreach)
library(doFuture)
library(survival)
```

```{r}
# train.surv <- ...
model_base <- coxph(model$formula, data = acc_surv %>% select(-addr_state,-home_ownership), x = T,y=T)

model_aug <- coxph(Surv(actual_duration_to_term, status) ~ ., data = acc_surv %>% select(-addr_state,-home_ownership), x = T,y=T)

score <- Score(list("Base model"=model_base,"Augmented Model"=model_aug),
              formula=Surv(actual_duration_to_term, status)~1,
              data=acc_surv, conf.int=F,
              times=seq(0.1,0.9,0.2),
              plots=c("calibration","ROC"),
              progress.bar=1)

# internal cross-validation
CVscore <- Score(list("Base model"=model_base,"Augmented Model"=model_aug),
              formula=Surv(actual_duration_to_term, status)~1,
              data=acc_surv %>% select(-addr_state,-home_ownership), conf.int=F,
              times=c(0.5),
              #times=seq(0.1,0.9,0.2),
              plots="calibration",
              split.method="bootcv",B=10,M=100,seed=2021,
              parallel="multicore",
              progress.bar=1)
```

Si decide di prendere il time-point a met√† del termine e si valutano le performance dei modelli sui tre aspetti: calibrazione, discriminazione, net benefit.

Per valutare la calibrazione dei modelli si usano insieme il Calibration Plot e il Brier Score.

```{r}
plotCalibration(score,times=0.5, cens.method="local",method="quantile", q=20, auc.in.legend = F)
title(main="Calibration at half term")

plotCalibration(CVscore,times=5,cens.method="local",method="quantile",q=10, auc.in.legend = F)
title(main="Calibration at half term")
```

### Discrimination

```{r}
plotROC(score,times=0.5,cens.method="local")
title(main="time-dependent ROC at half term")
```

```{r}
CVscore$AUC$score %>% filter(times == 0.5, model %in% c("Base Model", "Augmented Model")) %>% mutate(C.I = paste0("[", round(lower, 2), ",", round(upper,2), "]")) %>% select(model, AUC, C.I) %>% pander()
```

### Net Benefit

Quanto appena osservato ci indica che entrambi i modelli sono molto simili in quanto a calibrazione e capacit√† di discriminazione. \
Per arrivare infine a una valutazione dei modelli che ci consenta di affermare quale dei due sia migliore si stima il Net Benefit di ciascun modello e si confrontano i valori cos√¨ ottenuti.


```{r include=FALSE}
source("data_preparation/stdca.R")

fit_base<-survfit(model_base,newdata=d)
d$riskdeath_base<- 1-as.numeric(summary(fit_base,times=5)$surv)
fit_aug<-survfit(model_aug,newdata=d)
d$riskdeath_aug<- 1-as.numeric(summary(fit_aug,times=5)$surv)

netbenefit <- stdca(data = d, outcome = "status", ttoutcome = "fuyrs", timepoint = 5, xstop = 0.5, 
                    predictors = c("riskdeath_base","riskdeath_aug"))
```

```{r}
par(mar = c(5, 4.5, 4, 1)) 

# Net Benefit for model_base:
plot(netbenefit$net.benefit$threshold, netbenefit$net.benefit$riskdeath_base, type = "l", lwd = 3, 
     xlim = c(0, 1), ylim = c(0, 0.2), xlab = "Riskdeath", ylab = "Net Benefit", xaxt = "n", yaxt = "n", 
     cex.lab = 1.7, cex.axis = 1.6, frame = F)
axis(1, at = seq(0,1,0.1), labels = NA, pos = 0)
axis(1, at = seq(0,1,0.1), labels = seq(0,1,0.1), cex.axis = 1.7, pos = 0)
axis(2, at = c(0,0.05,0.1,0.15,0.2), labels = c(0,0.05,0.1,0.15,0.2), cex.axis = 1.7, pos = 0)

# Net Benefit for model_aug:
lines(netbenefit$net.benefit$threshold, netbenefit$net.benefit$riskdeath_aug, type = "l", lty = 3, lwd = 3) 

# We add an extra axis on the top of the graph with the costs:
axis(3, at = c(0,0.2,0.4,0.6,0.8,1), labels = c(0,0.25,0.67,1.5,4,'Inf'), cex.axis = 1.6)
mtext("Cost", side = 3, line = 2.5, cex = 1.7, las = 1)

legend('topright', c("Base Model","Augmented Model"), lwd = c(3, 3), lty = c(1, 3), col = c(1, 1), bty = 'n', cex = 1.7)
```


# Tuning



# Conclusions


```{r}
score <- Score(list("LogReg"=fit.glm,
                    "LogReg PCA"=fit.pca,
                    "LogRidge"=fit.ridge.cv,
                    "LogRegLasso"=fit.lasso.cv,
                    "RandFor"=fit.rf.150t),
              formula=y~1,
              data=test, conf.int=F,
              plots=c("calibration","ROC"),
              progress.bar=1)

```


```{r fig.height=6, fig.width=10}
plot_precision_vs_npred(test$y, list("glm"=yhat.glm, "pca"=yhat.pca, "ridge"=yhat.ridge, "lasso"=yhat.lasso,"RandomForest"=yhat.rf[,2]), title = "Results")
```

```{r}
stopCluster(cl)
```



